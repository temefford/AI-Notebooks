{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, I experiment with ReLoRa: a framework that enables pre-training with low-rank adapters that I described here:.\n",
        "\n",
        "*Last update: July 19th 2023. If something doesn't work anymore, you will have to retrieve versions available at that date. Please also comment about the issue on the related post so that I can fix the notebook: *"
      ],
      "metadata": {
        "id": "Fz62dsM1m6Aq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNnLR9u7YSGQ",
        "outputId": "852be409-e782-4a42-dbe6-bbbc488421c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'peft_pretraining'...\n",
            "remote: Enumerating objects: 635, done.\u001b[K\n",
            "remote: Counting objects: 100% (61/61), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 635 (delta 43), reused 40 (delta 29), pack-reused 574\u001b[K\n",
            "Receiving objects: 100% (635/635), 1.20 MiB | 23.15 MiB/s, done.\n",
            "Resolving deltas: 100% (398/398), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Guitaricet/peft_pretraining.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpncjV-LYeUV",
        "outputId": "ca0d855f-1df9-47bd-df37-5af35ad3ce30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/peft_pretraining\n"
          ]
        }
      ],
      "source": [
        "%cd peft_pretraining/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install all the requirements:"
      ],
      "metadata": {
        "id": "_i39SOrVpI3Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItKXlVUbYkVJ",
        "outputId": "302664d0-f154-4ae9-fc39-bdadf14566ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.0.1+cu118)\n",
            "Collecting transformers (from -r requirements.txt (line 2))\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers (from -r requirements.txt (line 3))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets (from -r requirements.txt (line 4))\n",
            "  Downloading datasets-2.13.1-py3-none-any.whl (486 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft (from -r requirements.txt (line 5))\n",
            "  Downloading peft-0.4.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb (from -r requirements.txt (line 6))\n",
            "  Downloading wandb-0.15.5-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting loguru (from -r requirements.txt (line 7))\n",
            "  Downloading loguru-0.7.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvitop (from -r requirements.txt (line 8))\n",
            "  Downloading nvitop-1.1.2-py3-none-any.whl (206 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.8/206.8 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lion-pytorch (from -r requirements.txt (line 9))\n",
            "  Downloading lion_pytorch-0.1.2-py3-none-any.whl (4.4 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (3.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r requirements.txt (line 1)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r requirements.txt (line 1)) (16.0.6)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers->-r requirements.txt (line 2))\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 2)) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 2)) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 2)) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 2)) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 2)) (2.27.1)\n",
            "Collecting safetensors>=0.3.1 (from transformers->-r requirements.txt (line 2))\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 2)) (4.65.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 4)) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets->-r requirements.txt (line 4))\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 4)) (1.5.3)\n",
            "Collecting xxhash (from datasets->-r requirements.txt (line 4))\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets->-r requirements.txt (line 4))\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 4)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 4)) (3.8.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft->-r requirements.txt (line 5)) (5.9.5)\n",
            "Collecting accelerate (from peft->-r requirements.txt (line 5))\n",
            "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 6)) (8.1.4)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb->-r requirements.txt (line 6))\n",
            "  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0 (from wandb->-r requirements.txt (line 6))\n",
            "  Downloading sentry_sdk-1.28.1-py2.py3-none-any.whl (214 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.7/214.7 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb->-r requirements.txt (line 6))\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools (from wandb->-r requirements.txt (line 6))\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb->-r requirements.txt (line 6))\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 6)) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 6)) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 6)) (3.20.3)\n",
            "Collecting nvidia-ml-py<11.526.0a0,>=11.450.51 (from nvitop->-r requirements.txt (line 8))\n",
            "  Downloading nvidia_ml_py-11.525.131-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: cachetools>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from nvitop->-r requirements.txt (line 8)) (5.3.1)\n",
            "Requirement already satisfied: termcolor>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from nvitop->-r requirements.txt (line 8)) (2.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 10)) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 10)) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 10)) (4.41.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 10)) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 10)) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 10)) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 10)) (2.8.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb->-r requirements.txt (line 6)) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (1.3.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 6))\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 2)) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 2)) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 2)) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 1)) (2.1.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 4)) (2022.7.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 1)) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 6))\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=3e4dfc86e3c56fd9fe7e35bc47bba295725cd25281c62f86b8ff49dcb8313fe1\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: tokenizers, safetensors, pathtools, nvidia-ml-py, xxhash, smmap, setproctitle, sentry-sdk, nvitop, loguru, docker-pycreds, dill, multiprocess, huggingface-hub, gitdb, transformers, GitPython, wandb, datasets, accelerate, peft, lion-pytorch\n",
            "Successfully installed GitPython-3.1.32 accelerate-0.21.0 datasets-2.13.1 dill-0.3.6 docker-pycreds-0.4.0 gitdb-4.0.10 huggingface-hub-0.16.4 lion-pytorch-0.1.2 loguru-0.7.0 multiprocess-0.70.14 nvidia-ml-py-11.525.131 nvitop-1.1.2 pathtools-0.1.2 peft-0.4.0 safetensors-0.3.1 sentry-sdk-1.28.1 setproctitle-1.3.2 smmap-5.0.0 tokenizers-0.13.3 transformers-4.31.0 wandb-0.15.5 xxhash-3.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since training takes a lot of time, I recommend first trying the framework with hyper-parameters that will stop the training early and shorten the validation (which can take many hours on consumer hardware).\n",
        "\n",
        "The framework doesn't have the option to shorten the validation, so we will have to do it manually. Open the file torchrun_main.py, and replace the line:\n",
        "\n",
        "*if evaluated_on_tokens > target_eval_tokens:*\n",
        "\n",
        "\n",
        "Note: At the time of writing this article, this line is line 129.\n",
        "by:\n",
        "\n",
        "\n",
        "*if evaluated_on_tokens > target_eval_tokens or total_batches > 10:*"
      ],
      "metadata": {
        "id": "lXAWj5YjpWV6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step one: We initialize pre-training without peft and without LoRa"
      ],
      "metadata": {
        "id": "B46hafADqR_C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cFqXbMZZD_f",
        "outputId": "e6d4a175-9ffb-47a4-f26d-f8cd1133d593"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-19 09:39:06.210043: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Starting script\n",
            "local rank: 0, device: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
            "\u001b[32m2023-07-19 09:39:14.105\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mUsing torch.distributed with rank 0 (only rank 0 will log)\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.106\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m192\u001b[0m - \u001b[1m****************************************\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.106\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m193\u001b[0m - \u001b[1mStarting training with the arguments\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.106\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1mmodel_config                   configs/llama_250m.json\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.106\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1muse_hf_model                   False\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.106\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1mcontinue_from                  None\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.107\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1mcontinue_from_peft             None\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.107\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1mrestore_optimizer              False\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.107\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1mbatch_size                     4\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.107\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1mgradient_accumulation          2\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.107\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1mtotal_batch_size               8\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.108\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1mmax_length                     512\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.108\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1muse_peft                       False\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.108\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1mlora_r                         None\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.108\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1mrelora                         None\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.108\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1mtrain_scaling                  False\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.109\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1mreset_optimizer_on_relora      True\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.109\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1moptimizer_random_pruning       0.0\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.109\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1moptimizer_magnitude_pruning    0.0\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.109\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1mforce_keep_original            False\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.109\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1mtrain_ln                       True\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.110\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1moptimizer                      Adam\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.110\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1mlr                             0.0005\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.110\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1mscheduler                      cosine\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.110\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1mcycle_length                   None\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.110\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1mrestart_warmup_steps           None\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.111\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1madjust_step                    0\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.111\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1mmin_lr_ratio                   0.1\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.111\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1mactivation_checkpointing       False\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.111\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1mweight_decay                   0.0\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.111\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1mwarmup_steps                   1000\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.112\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1meval_every                     1\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.112\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1mnum_training_steps             2\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.112\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1mmax_train_tokens               None\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.112\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1msave_every                     10\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.112\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1msave_dir                       checkpoints/llama_250m-2023-07-19-09-39-08\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.113\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1mtags                           ['warm_start_250M']\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.113\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1mdtype                          float32\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.113\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1mworkers                        1\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.113\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1mseed                           0\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.113\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1mlocal_rank                     0\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.114\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m196\u001b[0m - \u001b[1m****************************************\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:14.360\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m210\u001b[0m - \u001b[1mShuffling data with seed 42 (should be 42 for the first run and 42 + hash(checkpoint_path) for the runs that continue from a checkpoint)\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:17.955\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1m\n",
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(32000, 768, padding_idx=31999)\n",
            "    (layers): ModuleList(\n",
            "      (0-23): 24 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=768, out_features=2560, bias=False)\n",
            "          (down_proj): Linear(in_features=2560, out_features=768, bias=False)\n",
            "          (up_proj): Linear(in_features=768, out_features=2560, bias=False)\n",
            "          (act_fn): SiLUActivation()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm()\n",
            "        (post_attention_layernorm): LlamaRMSNorm()\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=32000, bias=False)\n",
            ")\n",
            "\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:17.956\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m332\u001b[0m - \u001b[1mTotal params before LoRA: 247.37M\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:17.956\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m333\u001b[0m - \u001b[1mTotal params after  LoRA: 247.37M\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:17.958\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m334\u001b[0m - \u001b[1mTrainable params: 247.37M\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:17.958\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m336\u001b[0m - \u001b[1mSaving model to checkpoints/llama_250m-2023-07-19-09-39-08 every 10 update steps\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "Update steps:   0%|                                       | 0/2 [00:00<?, ?it/s]Assigning 1024 shards (or data sources) of the dataset to each node.\n",
            "Update steps:  50%|███████████████▌               | 1/2 [00:02<00:02,  2.43s/it]\u001b[32m2023-07-19 09:39:23.399\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m560\u001b[0m - \u001b[1mPerforming evaluation at step 1\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:23.694\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m111\u001b[0m - \u001b[1mLoaded validation dataset in 0.29 seconds\u001b[0m\n",
            "1\n",
            "0\n",
            "\u001b[32m2023-07-19 09:39:23.697\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1mEval set prepared in 0.30 seconds\u001b[0m\n",
            "Assigning 8 shards (or data sources) of the dataset to each node.\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "\u001b[32m2023-07-19 09:39:29.521\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m571\u001b[0m - \u001b[1mEval loss at step 1: 9.572612762451172\u001b[0m\n",
            "Update steps: 100%|███████████████████████████████| 2/2 [00:10<00:00,  5.49s/it]\u001b[32m2023-07-19 09:39:30.948\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m560\u001b[0m - \u001b[1mPerforming evaluation at step 2\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:31.126\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m111\u001b[0m - \u001b[1mLoaded validation dataset in 0.18 seconds\u001b[0m\n",
            "1\n",
            "0\n",
            "\u001b[32m2023-07-19 09:39:31.174\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1mEval set prepared in 0.23 seconds\u001b[0m\n",
            "Assigning 8 shards (or data sources) of the dataset to each node.\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "\u001b[32m2023-07-19 09:39:35.423\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m571\u001b[0m - \u001b[1mEval loss at step 2: 9.571349143981934\u001b[0m\n",
            "Update steps: 3it [00:15,  5.68s/it]                                            \u001b[32m2023-07-19 09:39:36.842\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m560\u001b[0m - \u001b[1mPerforming evaluation at step 3\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:37.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m111\u001b[0m - \u001b[1mLoaded validation dataset in 0.19 seconds\u001b[0m\n",
            "1\n",
            "0\n",
            "\u001b[32m2023-07-19 09:39:37.070\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1mEval set prepared in 0.23 seconds\u001b[0m\n",
            "Assigning 8 shards (or data sources) of the dataset to each node.\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "\u001b[32m2023-07-19 09:39:42.509\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m571\u001b[0m - \u001b[1mEval loss at step 3: 9.571348190307617\u001b[0m\n",
            "\u001b[32m2023-07-19 09:39:42.522\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mReached max number of update steps (f2). Stopping training.\u001b[0m\n",
            "Rank 0 stopping training.\n",
            "\u001b[32m2023-07-19 09:39:42.601\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m602\u001b[0m - \u001b[1mTraining finished\u001b[0m\n",
            "Update steps: 3it [00:21,  7.25s/it]\n",
            "\u001b[32m2023-07-19 09:39:42.603\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m607\u001b[0m - \u001b[1mSaving model and optimizer to checkpoints/llama_250m-2023-07-19-09-39-08/model_3, update step 3\u001b[0m\n",
            "\u001b[32m2023-07-19 09:40:03.952\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m634\u001b[0m - \u001b[1mRunning final evaluation\u001b[0m\n",
            "\u001b[32m2023-07-19 09:40:04.501\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m111\u001b[0m - \u001b[1mLoaded validation dataset in 0.21 seconds\u001b[0m\n",
            "1\n",
            "0\n",
            "\u001b[32m2023-07-19 09:40:04.503\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1mEval set prepared in 0.21 seconds\u001b[0m\n",
            "Assigning 8 shards (or data sources) of the dataset to each node.\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "\u001b[32m2023-07-19 09:40:08.920\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m651\u001b[0m - \u001b[1mFinal eval loss: 9.5713472366333\u001b[0m\n",
            "\u001b[32m2023-07-19 09:40:08.921\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m653\u001b[0m - \u001b[1mScript finished successfully\u001b[0m\n",
            "Rank 0 finished successfully\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     final_eval_loss █▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   final_eval_tokens ▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                loss ▁█▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr █▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     n_lora_restarts ▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  throughput_batches ▁██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: throughput_examples ▁██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   throughput_tokens ▁▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         tokens_seen ▁▄█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         update_step ▁▅█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     final_eval_loss 9.57135\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   final_eval_tokens 11888\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                loss 10.51621\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     n_lora_restarts 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  throughput_batches 1.41077\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: throughput_examples 5.64308\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   throughput_tokens 1449.56696\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         tokens_seen 7213\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         update_step 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can sync this run to the cloud by running:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[1mwandb sync /content/peft_pretraining/wandb/offline-run-20230719_093914-41hfmqrx\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/offline-run-20230719_093914-41hfmqrx/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!torchrun --nproc-per-node 1 torchrun_main.py \\\n",
        "    --model_config configs/llama_250m.json \\\n",
        "    --batch_size 4 \\\n",
        "    --total_batch_size 8 \\\n",
        "    --lr 5e-4 \\\n",
        "    --max_length 512 \\\n",
        "    --tags warm_start_250M \\\n",
        "    --save_every 10 \\\n",
        "    --num_training_steps 2 \\\n",
        "    --workers 1 \\\n",
        "    --eval_every 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step two: Perform ReLoRa"
      ],
      "metadata": {
        "id": "WsctwxOFqYB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!torchrun --nproc-per-node 1 torchrun_main.py \\\n",
        "    --model_config configs/llama_250m.json \\\n",
        "    --batch_size 4 \\\n",
        "    --total_batch_size 8 \\\n",
        "    --lr 1e-3 \\\n",
        "    --max_length 512 \\\n",
        "    --use_peft \\\n",
        "    --relora 5 \\\n",
        "    --cycle_length 5 \\\n",
        "    --restart_warmup_steps 10 \\\n",
        "    --scheduler cosine_restarts \\\n",
        "    --warmup_steps 2 \\\n",
        "    --reset_optimizer_on_relora True \\\n",
        "    --num_training_steps 50 \\\n",
        "    --save_every 10 \\\n",
        "    --eval_every 10 \\\n",
        "    --continue_from checkpoints/llama_250m-2023-07-19-09-39-08/model_3 \\ #change this line to the name of your checkpoints\n",
        "    --tags relora_250M"
      ],
      "metadata": {
        "id": "ErofZ9jFa1C8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f5e0f52-010d-4596-ccc2-0636f939cd12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-19 09:58:37.992710: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Starting script\n",
            "local rank: 0, device: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
            "\u001b[32m2023-07-19 09:58:43.765\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m190\u001b[0m - \u001b[1mUsing torch.distributed with rank 0 (only rank 0 will log)\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.766\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1m****************************************\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.766\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m192\u001b[0m - \u001b[1mStarting training with the arguments\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.766\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mmodel_config                   configs/llama_250m.json\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.766\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1muse_hf_model                   False\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.766\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mcontinue_from                  checkpoints/llama_250m-2023-07-19-09-39-08/model_3\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.766\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mcontinue_from_peft             None\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.767\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mrestore_optimizer              False\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.767\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mbatch_size                     4\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.767\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mgradient_accumulation          2\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.767\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mtotal_batch_size               8\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.767\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mmax_length                     512\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.767\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1muse_peft                       True\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.767\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mlora_r                         128\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.767\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mrelora                         5\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.768\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mtrain_scaling                  False\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.768\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mreset_optimizer_on_relora      True\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.768\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1moptimizer_random_pruning       0.0\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.768\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1moptimizer_magnitude_pruning    0.0\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.768\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mforce_keep_original            False\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.768\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mtrain_ln                       True\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.768\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1moptimizer                      Adam\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mlr                             0.001\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mscheduler                      cosine_restarts\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mcycle_length                   5\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mrestart_warmup_steps           10\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1madjust_step                    0\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mmin_lr_ratio                   0.1\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mactivation_checkpointing       False\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.770\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mweight_decay                   0.0\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.770\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mwarmup_steps                   2\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.770\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1meval_every                     10\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.770\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mnum_training_steps             50\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.770\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mmax_train_tokens               None\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.770\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1msave_every                     10\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.770\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1msave_dir                       checkpoints/llama_250m-2023-07-19-09-58-39\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.771\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mtags                           ['relora_250M']\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.771\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mdtype                          float32\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.771\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mworkers                        8\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.771\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mseed                           0\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.771\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mlocal_rank                     0\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:43.771\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1m****************************************\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:44.291\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mShuffling data with seed 19219046 (should be 42 for the first run and 42 + hash(checkpoint_path) for the runs that continue from a checkpoint)\u001b[0m\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\u001b[32m2023-07-19 09:58:49.114\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1m****************************************\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:49.115\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m248\u001b[0m - \u001b[1mLoading model from checkpoints/llama_250m-2023-07-19-09-39-08/model_3\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:53.337\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m251\u001b[0m - \u001b[1mModel successfully loaded (strict=True policy)\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:53.338\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m254\u001b[0m - \u001b[1mLoading training state like global_step, update_step, and tokens_seen from checkpoints/llama_250m-2023-07-19-09-39-08/model_3\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:53.338\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m261\u001b[0m - \u001b[1mglobal_step       : 7\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:53.339\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m262\u001b[0m - \u001b[1mupdate_step       : 3\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:53.339\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m263\u001b[0m - \u001b[1mtokens_seen       : 7213\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:53.339\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m264\u001b[0m - \u001b[1mtokens_seen_before: 7213\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:53.340\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m265\u001b[0m - \u001b[1mWill train for 47 update steps\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:53.340\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m268\u001b[0m - \u001b[1m****************************************\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:55.367\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m330\u001b[0m - \u001b[1m\n",
            "ReLoRaModel(\n",
            "  (wrapped_model): LlamaForCausalLM(\n",
            "    (model): LlamaModel(\n",
            "      (embed_tokens): Embedding(32000, 768, padding_idx=31999)\n",
            "      (layers): ModuleList(\n",
            "        (0-23): 24 x LlamaDecoderLayer(\n",
            "          (self_attn): LlamaAttention(\n",
            "            (q_proj): ReLoRaLinear(\n",
            "              in_features=768, out_features=768, bias=False\n",
            "              (lora_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
            "              (lora_B): Linear(in_features=128, out_features=768, bias=False)\n",
            "            )\n",
            "            (k_proj): ReLoRaLinear(\n",
            "              in_features=768, out_features=768, bias=False\n",
            "              (lora_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
            "              (lora_B): Linear(in_features=128, out_features=768, bias=False)\n",
            "            )\n",
            "            (v_proj): ReLoRaLinear(\n",
            "              in_features=768, out_features=768, bias=False\n",
            "              (lora_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
            "              (lora_B): Linear(in_features=128, out_features=768, bias=False)\n",
            "            )\n",
            "            (o_proj): ReLoRaLinear(\n",
            "              in_features=768, out_features=768, bias=False\n",
            "              (lora_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
            "              (lora_B): Linear(in_features=128, out_features=768, bias=False)\n",
            "            )\n",
            "            (rotary_emb): LlamaRotaryEmbedding()\n",
            "          )\n",
            "          (mlp): LlamaMLP(\n",
            "            (gate_proj): ReLoRaLinear(\n",
            "              in_features=768, out_features=2560, bias=False\n",
            "              (lora_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
            "              (lora_B): Linear(in_features=128, out_features=2560, bias=False)\n",
            "            )\n",
            "            (down_proj): ReLoRaLinear(\n",
            "              in_features=2560, out_features=768, bias=False\n",
            "              (lora_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (lora_A): Linear(in_features=2560, out_features=128, bias=False)\n",
            "              (lora_B): Linear(in_features=128, out_features=768, bias=False)\n",
            "            )\n",
            "            (up_proj): ReLoRaLinear(\n",
            "              in_features=768, out_features=2560, bias=False\n",
            "              (lora_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
            "              (lora_B): Linear(in_features=128, out_features=2560, bias=False)\n",
            "            )\n",
            "            (act_fn): SiLUActivation()\n",
            "          )\n",
            "          (input_layernorm): LlamaRMSNorm()\n",
            "          (post_attention_layernorm): LlamaRMSNorm()\n",
            "        )\n",
            "      )\n",
            "      (norm): LlamaRMSNorm()\n",
            "    )\n",
            "    (lm_head): Linear(in_features=768, out_features=32000, bias=False)\n",
            "  )\n",
            ")\n",
            "\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:55.367\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1mTotal params before LoRA: 247.37M\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:55.368\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m332\u001b[0m - \u001b[1mTotal params after  LoRA: 296.92M\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:55.370\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m333\u001b[0m - \u001b[1mTrainable params: 98.73M\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:55.370\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m335\u001b[0m - \u001b[1mSaving model to checkpoints/llama_250m-2023-07-19-09-58-39 every 10 update steps\u001b[0m\n",
            "\u001b[32m2023-07-19 09:58:58.517\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m380\u001b[0m - \u001b[33m\u001b[1mPEFT config (all but lora_r) is hardcoded!\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "Update steps:   0%|                                      | 0/47 [00:00<?, ?it/s]Assigning 1024 shards (or data sources) of the dataset to each node.\n",
            "Update steps:   6%|█▉                            | 3/47 [00:06<01:29,  2.03s/it]\u001b[32m2023-07-19 09:59:04.978\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m503\u001b[0m - \u001b[1mPerforming lora reset. Current lr is 0.0003\u001b[0m\n",
            "\u001b[32m2023-07-19 09:59:05.041\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m508\u001b[0m - \u001b[1mResetting optimizer states to zeros\u001b[0m\n",
            "\u001b[32m2023-07-19 09:59:05.085\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m519\u001b[0m - \u001b[33m\u001b[1mOptimizer lr after the reset is large. This can lead to instability. Current lr is 0.0003\u001b[0m\n",
            "Update steps:   9%|██▌                           | 4/47 [00:08<01:22,  1.93s/it]\u001b[32m2023-07-19 09:59:06.752\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m556\u001b[0m - \u001b[1mFirst step after lora reset lr is 0.0004\u001b[0m\n",
            "Update steps:  15%|████▍                         | 7/47 [00:13<01:12,  1.81s/it]\u001b[32m2023-07-19 09:59:12.020\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m472\u001b[0m - \u001b[1mSaving model and optimizer to checkpoints/llama_250m-2023-07-19-09-58-39/model_10, update step 10\u001b[0m\n",
            "\u001b[32m2023-07-19 09:59:19.568\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m559\u001b[0m - \u001b[1mPerforming evaluation at step 10\u001b[0m\n",
            "\u001b[32m2023-07-19 09:59:19.777\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m111\u001b[0m - \u001b[1mLoaded validation dataset in 0.21 seconds\u001b[0m\n",
            "\u001b[32m2023-07-19 09:59:19.779\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m126\u001b[0m - \u001b[1mEval set prepared in 0.21 seconds\u001b[0m\n",
            "Assigning 8 shards (or data sources) of the dataset to each node.\n",
            "\u001b[32m2023-07-19 09:59:25.263\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m570\u001b[0m - \u001b[1mEval loss at step 10: 9.21914005279541\u001b[0m\n",
            "Update steps:  17%|█████                         | 8/47 [00:28<03:52,  5.97s/it]\u001b[32m2023-07-19 09:59:26.914\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m503\u001b[0m - \u001b[1mPerforming lora reset. Current lr is 0.00029283556748183927\u001b[0m\n",
            "\u001b[32m2023-07-19 09:59:26.977\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m508\u001b[0m - \u001b[1mResetting optimizer states to zeros\u001b[0m\n",
            "\u001b[32m2023-07-19 09:59:27.018\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m519\u001b[0m - \u001b[33m\u001b[1mOptimizer lr after the reset is large. This can lead to instability. Current lr is 0.00029283556748183927\u001b[0m\n",
            "Update steps:  19%|█████▋                        | 9/47 [00:30<02:57,  4.66s/it]\u001b[32m2023-07-19 09:59:28.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m556\u001b[0m - \u001b[1mFirst step after lora reset lr is 0.0003904474233091191\u001b[0m\n",
            "Update steps:  28%|████████                     | 13/47 [00:37<01:23,  2.45s/it]\u001b[32m2023-07-19 09:59:35.829\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m503\u001b[0m - \u001b[1mPerforming lora reset. Current lr is 0.0002721027009393167\u001b[0m\n",
            "\u001b[32m2023-07-19 09:59:35.895\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m508\u001b[0m - \u001b[1mResetting optimizer states to zeros\u001b[0m\n",
            "\u001b[32m2023-07-19 09:59:35.942\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m519\u001b[0m - \u001b[33m\u001b[1mOptimizer lr after the reset is large. This can lead to instability. Current lr is 0.0002721027009393167\u001b[0m\n",
            "Update steps:  30%|████████▋                    | 14/47 [00:39<01:14,  2.26s/it]\u001b[32m2023-07-19 09:59:37.633\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m556\u001b[0m - \u001b[1mFirst step after lora reset lr is 0.00036280360125242234\u001b[0m\n",
            "Update steps:  36%|██████████▍                  | 17/47 [00:44<00:58,  1.96s/it]\u001b[32m2023-07-19 09:59:43.052\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m472\u001b[0m - \u001b[1mSaving model and optimizer to checkpoints/llama_250m-2023-07-19-09-58-39/model_20, update step 20\u001b[0m\n",
            "\u001b[32m2023-07-19 09:59:54.923\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m559\u001b[0m - \u001b[1mPerforming evaluation at step 20\u001b[0m\n",
            "\u001b[32m2023-07-19 09:59:55.336\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m111\u001b[0m - \u001b[1mLoaded validation dataset in 0.41 seconds\u001b[0m\n",
            "\u001b[32m2023-07-19 09:59:55.338\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m126\u001b[0m - \u001b[1mEval set prepared in 0.41 seconds\u001b[0m\n",
            "Assigning 8 shards (or data sources) of the dataset to each node.\n",
            "\u001b[32m2023-07-19 10:00:02.113\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m570\u001b[0m - \u001b[1mEval loss at step 20: 8.312496185302734\u001b[0m\n",
            "Update steps:  38%|███████████                  | 18/47 [01:05<03:41,  7.62s/it]\u001b[32m2023-07-19 10:00:03.844\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m503\u001b[0m - \u001b[1mPerforming lora reset. Current lr is 0.0002400019814576463\u001b[0m\n",
            "\u001b[32m2023-07-19 10:00:03.914\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m508\u001b[0m - \u001b[1mResetting optimizer states to zeros\u001b[0m\n",
            "\u001b[32m2023-07-19 10:00:03.961\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m519\u001b[0m - \u001b[33m\u001b[1mOptimizer lr after the reset is large. This can lead to instability. Current lr is 0.0002400019814576463\u001b[0m\n",
            "Update steps:  40%|███████████▋                 | 19/47 [01:07<02:44,  5.87s/it]\u001b[32m2023-07-19 10:00:05.642\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m556\u001b[0m - \u001b[1mFirst step after lora reset lr is 0.00032000264194352845\u001b[0m\n",
            "Update steps:  49%|██████████████▏              | 23/47 [01:14<01:07,  2.81s/it]\u001b[32m2023-07-19 10:00:12.999\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m503\u001b[0m - \u001b[1mPerforming lora reset. Current lr is 0.00019994057108884032\u001b[0m\n",
            "\u001b[32m2023-07-19 10:00:13.068\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m508\u001b[0m - \u001b[1mResetting optimizer states to zeros\u001b[0m\n",
            "\u001b[32m2023-07-19 10:00:13.109\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m519\u001b[0m - \u001b[33m\u001b[1mOptimizer lr after the reset is large. This can lead to instability. Current lr is 0.00019994057108884032\u001b[0m\n",
            "Update steps:  51%|██████████████▊              | 24/47 [01:16<00:57,  2.51s/it]\u001b[32m2023-07-19 10:00:14.811\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m556\u001b[0m - \u001b[1mFirst step after lora reset lr is 0.00026658742811845376\u001b[0m\n",
            "Update steps:  57%|████████████████▋            | 27/47 [01:21<00:41,  2.07s/it]\u001b[32m2023-07-19 10:00:20.313\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m472\u001b[0m - \u001b[1mSaving model and optimizer to checkpoints/llama_250m-2023-07-19-09-58-39/model_30, update step 30\u001b[0m\n",
            "\u001b[32m2023-07-19 10:00:34.182\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m559\u001b[0m - \u001b[1mPerforming evaluation at step 30\u001b[0m\n",
            "\u001b[32m2023-07-19 10:00:34.376\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m111\u001b[0m - \u001b[1mLoaded validation dataset in 0.19 seconds\u001b[0m\n",
            "\u001b[32m2023-07-19 10:00:34.378\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m126\u001b[0m - \u001b[1mEval set prepared in 0.19 seconds\u001b[0m\n",
            "Assigning 8 shards (or data sources) of the dataset to each node.\n",
            "\u001b[32m2023-07-19 10:00:39.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m570\u001b[0m - \u001b[1mEval loss at step 30: 7.471963405609131\u001b[0m\n",
            "Update steps:  60%|█████████████████▎           | 28/47 [01:43<02:28,  7.84s/it]\u001b[32m2023-07-19 10:00:41.624\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m503\u001b[0m - \u001b[1mPerforming lora reset. Current lr is 0.00015617057755393067\u001b[0m\n",
            "\u001b[32m2023-07-19 10:00:41.685\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m508\u001b[0m - \u001b[1mResetting optimizer states to zeros\u001b[0m\n",
            "\u001b[32m2023-07-19 10:00:41.729\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m519\u001b[0m - \u001b[33m\u001b[1mOptimizer lr after the reset is large. This can lead to instability. Current lr is 0.00015617057755393067\u001b[0m\n",
            "Update steps:  62%|█████████████████▉           | 29/47 [01:44<01:48,  6.04s/it]\u001b[32m2023-07-19 10:00:43.469\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m556\u001b[0m - \u001b[1mFirst step after lora reset lr is 0.00020822743673857424\u001b[0m\n",
            "Update steps:  70%|████████████████████▎        | 33/47 [01:52<00:40,  2.87s/it]\u001b[32m2023-07-19 10:00:50.943\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m503\u001b[0m - \u001b[1mPerforming lora reset. Current lr is 0.00011333773663071288\u001b[0m\n",
            "\u001b[32m2023-07-19 10:00:51.014\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m508\u001b[0m - \u001b[1mResetting optimizer states to zeros\u001b[0m\n",
            "\u001b[32m2023-07-19 10:00:51.060\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m519\u001b[0m - \u001b[33m\u001b[1mOptimizer lr after the reset is large. This can lead to instability. Current lr is 0.00011333773663071288\u001b[0m\n",
            "Update steps:  72%|████████████████████▉        | 34/47 [01:54<00:33,  2.57s/it]\u001b[32m2023-07-19 10:00:52.827\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m556\u001b[0m - \u001b[1mFirst step after lora reset lr is 0.00015111698217428385\u001b[0m\n",
            "Update steps:  79%|██████████████████████▊      | 37/47 [01:59<00:21,  2.13s/it]\u001b[32m2023-07-19 10:00:58.506\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m472\u001b[0m - \u001b[1mSaving model and optimizer to checkpoints/llama_250m-2023-07-19-09-58-39/model_40, update step 40\u001b[0m\n",
            "\u001b[32m2023-07-19 10:01:19.316\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m559\u001b[0m - \u001b[1mPerforming evaluation at step 40\u001b[0m\n",
            "\u001b[32m2023-07-19 10:01:19.600\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m111\u001b[0m - \u001b[1mLoaded validation dataset in 0.28 seconds\u001b[0m\n",
            "\u001b[32m2023-07-19 10:01:19.601\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m126\u001b[0m - \u001b[1mEval set prepared in 0.28 seconds\u001b[0m\n",
            "Assigning 8 shards (or data sources) of the dataset to each node.\n",
            "\u001b[32m2023-07-19 10:01:26.318\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m570\u001b[0m - \u001b[1mEval loss at step 40: 7.1128153800964355\u001b[0m\n",
            "Update steps:  81%|███████████████████████▍     | 38/47 [02:29<01:33, 10.37s/it]\u001b[32m2023-07-19 10:01:28.091\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m503\u001b[0m - \u001b[1mPerforming lora reset. Current lr is 7.598831496149069e-05\u001b[0m\n",
            "\u001b[32m2023-07-19 10:01:28.157\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m508\u001b[0m - \u001b[1mResetting optimizer states to zeros\u001b[0m\n",
            "\u001b[32m2023-07-19 10:01:28.200\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m519\u001b[0m - \u001b[33m\u001b[1mOptimizer lr after the reset is large. This can lead to instability. Current lr is 7.598831496149069e-05\u001b[0m\n",
            "Update steps:  83%|████████████████████████     | 39/47 [02:31<01:02,  7.82s/it]\u001b[32m2023-07-19 10:01:29.987\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m556\u001b[0m - \u001b[1mFirst step after lora reset lr is 0.00010131775328198761\u001b[0m\n",
            "Update steps:  91%|██████████████████████████▌  | 43/47 [02:38<00:13,  3.32s/it]\u001b[32m2023-07-19 10:01:37.544\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m503\u001b[0m - \u001b[1mPerforming lora reset. Current lr is 4.808657048910078e-05\u001b[0m\n",
            "\u001b[32m2023-07-19 10:01:37.613\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m508\u001b[0m - \u001b[1mResetting optimizer states to zeros\u001b[0m\n",
            "\u001b[32m2023-07-19 10:01:37.655\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m519\u001b[0m - \u001b[33m\u001b[1mOptimizer lr after the reset is large. This can lead to instability. Current lr is 4.808657048910078e-05\u001b[0m\n",
            "Update steps:  94%|███████████████████████████▏ | 44/47 [02:40<00:08,  2.90s/it]\u001b[32m2023-07-19 10:01:39.463\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m556\u001b[0m - \u001b[1mFirst step after lora reset lr is 6.411542731880104e-05\u001b[0m\n",
            "Update steps: 100%|█████████████████████████████| 47/47 [02:46<00:00,  2.25s/it]\u001b[32m2023-07-19 10:01:45.200\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m472\u001b[0m - \u001b[1mSaving model and optimizer to checkpoints/llama_250m-2023-07-19-09-58-39/model_50, update step 50\u001b[0m\n",
            "\u001b[32m2023-07-19 10:01:57.241\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m559\u001b[0m - \u001b[1mPerforming evaluation at step 50\u001b[0m\n",
            "\u001b[32m2023-07-19 10:01:57.444\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m111\u001b[0m - \u001b[1mLoaded validation dataset in 0.20 seconds\u001b[0m\n",
            "\u001b[32m2023-07-19 10:01:57.447\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m126\u001b[0m - \u001b[1mEval set prepared in 0.21 seconds\u001b[0m\n",
            "Assigning 8 shards (or data sources) of the dataset to each node.\n",
            "\u001b[32m2023-07-19 10:02:03.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m570\u001b[0m - \u001b[1mEval loss at step 50: 7.000484943389893\u001b[0m\n",
            "Update steps: 48it [03:06,  7.57s/it]                                           \u001b[32m2023-07-19 10:02:05.216\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m503\u001b[0m - \u001b[1mPerforming lora reset. Current lr is 3.259398714556389e-05\u001b[0m\n",
            "\u001b[32m2023-07-19 10:02:05.283\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m508\u001b[0m - \u001b[1mResetting optimizer states to zeros\u001b[0m\n",
            "\u001b[32m2023-07-19 10:02:05.310\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m519\u001b[0m - \u001b[33m\u001b[1mOptimizer lr after the reset is large. This can lead to instability. Current lr is 3.259398714556389e-05\u001b[0m\n",
            "\u001b[32m2023-07-19 10:02:05.323\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m446\u001b[0m - \u001b[1mReached max number of update steps (f50). Stopping training.\u001b[0m\n",
            "Rank 0 stopping training.\n",
            "\u001b[32m2023-07-19 10:02:05.767\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m601\u001b[0m - \u001b[1mTraining finished\u001b[0m\n",
            "Update steps: 48it [03:07,  3.90s/it]\n",
            "\u001b[32m2023-07-19 10:02:05.768\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m606\u001b[0m - \u001b[1mSaving model and optimizer to checkpoints/llama_250m-2023-07-19-09-58-39/model_51, update step 51\u001b[0m\n",
            "\u001b[32m2023-07-19 10:02:13.695\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m633\u001b[0m - \u001b[1mRunning final evaluation\u001b[0m\n",
            "\u001b[32m2023-07-19 10:02:14.389\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m111\u001b[0m - \u001b[1mLoaded validation dataset in 0.22 seconds\u001b[0m\n",
            "\u001b[32m2023-07-19 10:02:14.391\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m126\u001b[0m - \u001b[1mEval set prepared in 0.22 seconds\u001b[0m\n",
            "Assigning 8 shards (or data sources) of the dataset to each node.\n",
            "\u001b[32m2023-07-19 10:02:20.298\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m650\u001b[0m - \u001b[1mFinal eval loss: 6.9946746826171875\u001b[0m\n",
            "\u001b[32m2023-07-19 10:02:20.300\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m652\u001b[0m - \u001b[1mScript finished successfully\u001b[0m\n",
            "Rank 0 finished successfully\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     final_eval_loss █▅▃▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   final_eval_tokens ▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                loss ███▇█▇█▇▆▆▇▆▅▅▅▅▄▃▄▃▄▄▃▄▃▂▂▂▂▂▂▂▂▂▂▁▁▂▃▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr █▄▅▇▁▄▅▆▁▂▅▆▁▂▃▅▁▂▃▄▁▂▂▃▄▂▂▃▃▁▂▂▂▁▁▂▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     n_lora_restarts ▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  throughput_batches ▁████████████▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: throughput_examples ▁████████████▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   throughput_tokens ▁▇▆▆▅▆▃▇▆▆▅▆▄▇▇▅▆▇▇█▆▅▅▄▅█▇▅█▇▇▄▄▃▅▅▆▄▆▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         tokens_seen ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         update_step ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     final_eval_loss 6.99467\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   final_eval_tokens 11888\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                loss 7.91872\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 3e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     n_lora_restarts 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  throughput_batches 1.08435\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: throughput_examples 4.33741\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   throughput_tokens 1439.47638\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         tokens_seen 115793\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         update_step 51\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can sync this run to the cloud by running:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[1mwandb sync /content/peft_pretraining/wandb/offline-run-20230719_095843-3ui2sef0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/offline-run-20230719_095843-3ui2sef0/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we are sure that everything works, we can run ReLoRa pre-training with reasonable hyperparameters. *Note: I recommend removing \"total_batches > 10\" that we have added to speed up validation.*"
      ],
      "metadata": {
        "id": "rtwa8RkHmKzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torchrun --nproc-per-node 1 torchrun_main.py \\\n",
        "    --model_config configs/llama_250m.json \\\n",
        "    --batch_size 4 \\\n",
        "    --total_batch_size 8\\\n",
        "    --lr 5e-4 \\\n",
        "    --max_length 512 \\\n",
        "    --tags warm_start_250M \\\n",
        "    --save_every 1000 \\\n",
        "    --num_training_steps 10000\n",
        "\n",
        "torchrun --nproc-per-node 1 torchrun_main.py \\\n",
        "    --model_config configs/llama_250m.json \\\n",
        "    --batch_size 4 \\\n",
        "    --total_batch_size 8 \\\n",
        "    --lr 1e-3 \\\n",
        "    --max_length 512 \\\n",
        "    --use_peft \\\n",
        "    --relora 5000 \\\n",
        "    --cycle_length 5000 \\\n",
        "    --restart_warmup_steps 100 \\\n",
        "    --scheduler cosine_restarts \\\n",
        "    --warmup_steps 500 \\\n",
        "    --reset_optimizer_on_relora True \\\n",
        "    --num_training_steps 10000 \\\n",
        "    --save_every 5000 \\\n",
        "    --eval_every 5000 \\\n",
        "    --continue_from <your checkpoint from step one>\\ #change this line to the name of your checkpoints\n",
        "    --tags relora_250M"
      ],
      "metadata": {
        "id": "mQPaerPfqmMD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}