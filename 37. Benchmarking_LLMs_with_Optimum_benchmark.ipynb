{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook shows how to use optimum-benchmark to benchmark LLMs. It  focuses on benchmarking quantization algorithms for Mistral 7B."
      ],
      "metadata": {
        "id": "riGNbcKn-b5P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to install the following packages. bitsandbytes, auto-gptq and autoawq are only necessary if you benchmark models quantized with these algorithms."
      ],
      "metadata": {
        "id": "Ow56uKbJ0IGO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUJH8FMq-Yx_",
        "outputId": "7a41cb94-1df3-4323-8807-e8930638d244"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/optimum-benchmark.git\n",
            "  Cloning https://github.com/huggingface/optimum-benchmark.git to /tmp/pip-req-build-w7nad0pk\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/optimum-benchmark.git /tmp/pip-req-build-w7nad0pk\n",
            "  Resolved https://github.com/huggingface/optimum-benchmark.git to commit ef70214a33902d33896d4edd663e08480682c05f\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyrsmi@ git+https://github.com/RadeonOpenCompute/pyrsmi.git (from optimum-benchmark==0.0.1)\n",
            "  Cloning https://github.com/RadeonOpenCompute/pyrsmi.git to /tmp/pip-install-ixjgfm_5/pyrsmi_fe0ad5f50f4b4933bd5593e62a4c335e\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/RadeonOpenCompute/pyrsmi.git /tmp/pip-install-ixjgfm_5/pyrsmi_fe0ad5f50f4b4933bd5593e62a4c335e\n",
            "  Resolved https://github.com/RadeonOpenCompute/pyrsmi.git to commit 0c27f5c5bc6bbb6c3fd60b57cab67eb92f44b8e2\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting optimum>=1.14.0 (from optimum-benchmark==0.0.1)\n",
            "  Downloading optimum-1.16.1-py3-none-any.whl (403 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m403.3/403.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate (from optimum-benchmark==0.0.1)\n",
            "  Downloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hydra-colorlog (from optimum-benchmark==0.0.1)\n",
            "  Downloading hydra_colorlog-1.2.0-py3-none-any.whl (3.6 kB)\n",
            "Collecting hydra-core (from optimum-benchmark==0.0.1)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf (from optimum-benchmark==0.0.1)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from optimum-benchmark==0.0.1) (5.9.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from optimum-benchmark==0.0.1) (1.5.3)\n",
            "Collecting py3nvml>=0.2.7 (from optimum-benchmark==0.0.1)\n",
            "  Downloading py3nvml-0.2.7-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from optimum>=1.14.0->optimum-benchmark==0.0.1)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from optimum>=1.14.0->optimum-benchmark==0.0.1) (1.12)\n",
            "Requirement already satisfied: transformers[sentencepiece]>=4.26.0 in /usr/local/lib/python3.10/dist-packages (from optimum>=1.14.0->optimum-benchmark==0.0.1) (4.35.2)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from optimum>=1.14.0->optimum-benchmark==0.0.1) (2.1.0+cu121)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from optimum>=1.14.0->optimum-benchmark==0.0.1) (23.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optimum>=1.14.0->optimum-benchmark==0.0.1) (1.23.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from optimum>=1.14.0->optimum-benchmark==0.0.1) (0.20.2)\n",
            "Collecting datasets (from optimum>=1.14.0->optimum-benchmark==0.0.1)\n",
            "  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xmltodict (from py3nvml>=0.2.7->optimum-benchmark==0.0.1)\n",
            "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate->optimum-benchmark==0.0.1) (6.0.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate->optimum-benchmark==0.0.1) (0.4.1)\n",
            "Collecting colorlog (from hydra-colorlog->optimum-benchmark==0.0.1)\n",
            "  Downloading colorlog-6.8.0-py3-none-any.whl (11 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core->optimum-benchmark==0.0.1)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->optimum-benchmark==0.0.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->optimum-benchmark==0.0.1) (2023.3.post1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum>=1.14.0->optimum-benchmark==0.0.1) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum>=1.14.0->optimum-benchmark==0.0.1) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum>=1.14.0->optimum-benchmark==0.0.1) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum>=1.14.0->optimum-benchmark==0.0.1) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum>=1.14.0->optimum-benchmark==0.0.1) (4.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->optimum-benchmark==0.0.1) (1.16.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->optimum>=1.14.0->optimum-benchmark==0.0.1) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->optimum>=1.14.0->optimum-benchmark==0.0.1) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->optimum>=1.14.0->optimum-benchmark==0.0.1) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum>=1.14.0->optimum-benchmark==0.0.1) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum>=1.14.0->optimum-benchmark==0.0.1) (0.15.0)\n",
            "Collecting sentencepiece!=0.1.92,>=0.1.91 (from transformers[sentencepiece]>=4.26.0->optimum>=1.14.0->optimum-benchmark==0.0.1)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum>=1.14.0->optimum-benchmark==0.0.1) (3.20.3)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->optimum>=1.14.0->optimum-benchmark==0.0.1)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum>=1.14.0->optimum-benchmark==0.0.1) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->optimum>=1.14.0->optimum-benchmark==0.0.1) (0.6)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets->optimum>=1.14.0->optimum-benchmark==0.0.1)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->optimum>=1.14.0->optimum-benchmark==0.0.1) (3.4.1)\n",
            "Collecting multiprocess (from datasets->optimum>=1.14.0->optimum-benchmark==0.0.1)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->optimum>=1.14.0->optimum-benchmark==0.0.1) (3.9.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->optimum>=1.14.0->optimum-benchmark==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum>=1.14.0->optimum-benchmark==0.0.1) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum>=1.14.0->optimum-benchmark==0.0.1) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum>=1.14.0->optimum-benchmark==0.0.1) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum>=1.14.0->optimum-benchmark==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum>=1.14.0->optimum-benchmark==0.0.1) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum>=1.14.0->optimum-benchmark==0.0.1) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum>=1.14.0->optimum-benchmark==0.0.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum>=1.14.0->optimum-benchmark==0.0.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum>=1.14.0->optimum-benchmark==0.0.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum>=1.14.0->optimum-benchmark==0.0.1) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9->optimum>=1.14.0->optimum-benchmark==0.0.1) (2.1.3)\n",
            "Building wheels for collected packages: optimum-benchmark, antlr4-python3-runtime, pyrsmi\n",
            "  Building wheel for optimum-benchmark (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for optimum-benchmark: filename=optimum_benchmark-0.0.1-py3-none-any.whl size=77571 sha256=60f71911b8a4ee50e2a67dcb9738a59b1055efece3bccded902797da0370b713\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-v80855zn/wheels/48/c5/52/c5e1295ae5e4a012df46fd1e0fce40ca8547c6772c738bbcef\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=53ebb10f18ab2804faeb62a23f67400d480d173557dbf7a6c8bc2166a88913d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "  Building wheel for pyrsmi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyrsmi: filename=pyrsmi-1.0.2-py2.py3-none-any.whl size=8460 sha256=a8c09e527c82f89759598e21ce56c0331a5de70da08b426d3ed781010ca42d92\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-v80855zn/wheels/13/c5/a0/e6174d0cd8adf5c8bab06ac38e4aa9fadb904417f321401ad8\n",
            "Successfully built optimum-benchmark antlr4-python3-runtime pyrsmi\n",
            "Installing collected packages: sentencepiece, antlr4-python3-runtime, xmltodict, pyrsmi, omegaconf, humanfriendly, dill, colorlog, py3nvml, multiprocess, hydra-core, coloredlogs, hydra-colorlog, accelerate, datasets, optimum, optimum-benchmark\n",
            "Successfully installed accelerate-0.26.1 antlr4-python3-runtime-4.9.3 coloredlogs-15.0.1 colorlog-6.8.0 datasets-2.16.1 dill-0.3.7 humanfriendly-10.0 hydra-colorlog-1.2.0 hydra-core-1.3.2 multiprocess-0.70.15 omegaconf-2.3.0 optimum-1.16.1 optimum-benchmark-0.0.1 py3nvml-0.2.7 pyrsmi-1.0.2 sentencepiece-0.1.99 xmltodict-0.13.0\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->bitsandbytes) (1.23.5)\n",
            "Installing collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.42.0\n",
            "Collecting auto-gptq\n",
            "  Downloading auto_gptq-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: accelerate>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.26.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (2.16.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.1.99)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (1.23.5)\n",
            "Collecting rouge (from auto-gptq)\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Collecting gekko (from auto-gptq)\n",
            "  Downloading gekko-1.0.6-py3-none-any.whl (12.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (2.1.0+cu121)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.4.1)\n",
            "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (4.35.2)\n",
            "Collecting peft>=0.5.0 (from auto-gptq)\n",
            "  Downloading peft-0.7.1-py3-none-any.whl (168 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (4.66.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.22.0->auto-gptq) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.22.0->auto-gptq) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.22.0->auto-gptq) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.22.0->auto-gptq) (0.20.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (0.15.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (3.9.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge->auto-gptq) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2023.3.post1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->auto-gptq) (1.3.0)\n",
            "Installing collected packages: rouge, gekko, peft, auto-gptq\n",
            "Successfully installed auto-gptq-0.6.0 gekko-1.0.6 peft-0.7.1 rouge-1.0.1\n",
            "Collecting autoawq\n",
            "  Downloading autoawq-0.1.8-cp310-cp310-manylinux2014_x86_64.whl (20.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from autoawq) (2.1.0+cu121)\n",
            "Requirement already satisfied: transformers>=4.35.0 in /usr/local/lib/python3.10/dist-packages (from autoawq) (4.35.2)\n",
            "Requirement already satisfied: tokenizers>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from autoawq) (0.15.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from autoawq) (0.26.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from autoawq) (0.1.99)\n",
            "Collecting lm-eval (from autoawq)\n",
            "  Downloading lm_eval-0.4.0.tar.gz (457 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m457.1/457.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting texttable (from autoawq)\n",
            "  Downloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from autoawq) (0.10.2)\n",
            "Collecting attributedict (from autoawq)\n",
            "  Downloading attributedict-0.3.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from autoawq) (3.20.3)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from autoawq) (0.16.0+cu121)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from autoawq) (0.9.0)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.12.1->autoawq) (0.20.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->autoawq) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->autoawq) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->autoawq) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->autoawq) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->autoawq) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->autoawq) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->autoawq) (2.1.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.35.0->autoawq) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.35.0->autoawq) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.35.0->autoawq) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.35.0->autoawq) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.35.0->autoawq) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.35.0->autoawq) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.35.0->autoawq) (4.66.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->autoawq) (5.9.5)\n",
            "Collecting rootpath>=0.1.0 (from attributedict->autoawq)\n",
            "  Downloading rootpath-0.1.1-py3-none-any.whl (15 kB)\n",
            "Collecting inspecta>=0.1.0 (from attributedict->autoawq)\n",
            "  Downloading inspecta-0.1.3-py3-none-any.whl (9.2 kB)\n",
            "Collecting colour-runner>=0.0.5 (from attributedict->autoawq)\n",
            "  Downloading colour_runner-0.1.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Collecting deepdiff>=3.3.0 (from attributedict->autoawq)\n",
            "  Downloading deepdiff-6.7.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.6/76.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tox>=3.0.0 (from attributedict->autoawq)\n",
            "  Downloading tox-4.12.0-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coverage>=4.5.2 (from attributedict->autoawq)\n",
            "  Downloading coverage-7.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.2/233.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting codecov>=2.0.15 (from attributedict->autoawq)\n",
            "  Downloading codecov-2.1.13-py2.py3-none-any.whl (16 kB)\n",
            "Collecting evaluate (from lm-eval->autoawq)\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from lm-eval->autoawq) (2.16.1)\n",
            "Collecting jsonlines (from lm-eval->autoawq)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from lm-eval->autoawq) (2.8.8)\n",
            "Requirement already satisfied: peft>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from lm-eval->autoawq) (0.7.1)\n",
            "Collecting pybind11>=2.6.2 (from lm-eval->autoawq)\n",
            "  Downloading pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.7/227.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytablewriter (from lm-eval->autoawq)\n",
            "  Downloading pytablewriter-1.2.0-py3-none-any.whl (111 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.1/111.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rouge-score>=0.0.4 (from lm-eval->autoawq)\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sacrebleu>=1.5.0 (from lm-eval->autoawq)\n",
            "  Downloading sacrebleu-2.4.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from lm-eval->autoawq) (1.2.2)\n",
            "Collecting sqlitedict (from lm-eval->autoawq)\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tqdm-multiprocess (from lm-eval->autoawq)\n",
            "  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\n",
            "Collecting zstandard (from lm-eval->autoawq)\n",
            "  Downloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->autoawq) (9.4.0)\n",
            "Collecting blessings (from colour-runner>=0.0.5->attributedict->autoawq)\n",
            "  Downloading blessings-1.7-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from colour-runner>=0.0.5->attributedict->autoawq) (2.16.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->lm-eval->autoawq) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->lm-eval->autoawq) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->lm-eval->autoawq) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->lm-eval->autoawq) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->lm-eval->autoawq) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->lm-eval->autoawq) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->lm-eval->autoawq) (3.9.1)\n",
            "Collecting ordered-set<4.2.0,>=4.0.2 (from deepdiff>=3.3.0->attributedict->autoawq)\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Collecting responses<0.19 (from evaluate->lm-eval->autoawq)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from inspecta>=0.1.0->attributedict->autoawq) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from inspecta>=0.1.0->attributedict->autoawq) (2.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.35.0->autoawq) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.35.0->autoawq) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.35.0->autoawq) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.35.0->autoawq) (2023.11.17)\n",
            "Requirement already satisfied: coloredlogs>=10.0 in /usr/local/lib/python3.10/dist-packages (from rootpath>=0.1.0->attributedict->autoawq) (15.0.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm-eval->autoawq) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm-eval->autoawq) (3.8.1)\n",
            "Collecting portalocker (from sacrebleu>=1.5.0->lm-eval->autoawq)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Collecting colorama (from sacrebleu>=1.5.0->lm-eval->autoawq)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm-eval->autoawq) (4.9.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm-eval->autoawq) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm-eval->autoawq) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm-eval->autoawq) (3.2.0)\n",
            "Requirement already satisfied: cachetools>=5.3.2 in /usr/local/lib/python3.10/dist-packages (from tox>=3.0.0->attributedict->autoawq) (5.3.2)\n",
            "Requirement already satisfied: chardet>=5.2 in /usr/local/lib/python3.10/dist-packages (from tox>=3.0.0->attributedict->autoawq) (5.2.0)\n",
            "Requirement already satisfied: platformdirs>=4.1 in /usr/local/lib/python3.10/dist-packages (from tox>=3.0.0->attributedict->autoawq) (4.1.0)\n",
            "Requirement already satisfied: pluggy>=1.3 in /usr/local/lib/python3.10/dist-packages (from tox>=3.0.0->attributedict->autoawq) (1.3.0)\n",
            "Collecting pyproject-api>=1.6.1 (from tox>=3.0.0->attributedict->autoawq)\n",
            "  Downloading pyproject_api-1.6.1-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from tox>=3.0.0->attributedict->autoawq) (2.0.1)\n",
            "Collecting virtualenv>=20.25 (from tox>=3.0.0->attributedict->autoawq)\n",
            "  Downloading virtualenv-20.25.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.1->autoawq) (2.1.3)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines->lm-eval->autoawq) (23.2.0)\n",
            "Requirement already satisfied: setuptools>=38.3.0 in /usr/local/lib/python3.10/dist-packages (from pytablewriter->lm-eval->autoawq) (67.7.2)\n",
            "Collecting DataProperty<2,>=1.0.1 (from pytablewriter->lm-eval->autoawq)\n",
            "  Downloading DataProperty-1.0.1-py3-none-any.whl (27 kB)\n",
            "Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm-eval->autoawq)\n",
            "  Downloading mbstrdecoder-1.1.3-py3-none-any.whl (7.8 kB)\n",
            "Collecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm-eval->autoawq)\n",
            "  Downloading pathvalidate-3.2.0-py3-none-any.whl (23 kB)\n",
            "Collecting tabledata<2,>=1.3.1 (from pytablewriter->lm-eval->autoawq)\n",
            "  Downloading tabledata-1.3.3-py3-none-any.whl (11 kB)\n",
            "Collecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm-eval->autoawq)\n",
            "  Downloading tcolorpy-0.1.4-py3-none-any.whl (7.9 kB)\n",
            "Collecting typepy[datetime]<2,>=1.3.2 (from pytablewriter->lm-eval->autoawq)\n",
            "  Downloading typepy-1.3.2-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.1->autoawq) (1.3.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs>=10.0->rootpath>=0.1.0->attributedict->autoawq) (10.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->lm-eval->autoawq) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->lm-eval->autoawq) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->lm-eval->autoawq) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->lm-eval->autoawq) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->lm-eval->autoawq) (4.0.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm-eval->autoawq) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2018.9 in /usr/local/lib/python3.10/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm-eval->autoawq) (2023.3.post1)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv>=20.25->tox>=3.0.0->attributedict->autoawq)\n",
            "  Downloading distlib-0.3.8-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score>=0.0.4->lm-eval->autoawq) (8.1.7)\n",
            "Building wheels for collected packages: lm-eval, rouge-score, sqlitedict\n",
            "  Building wheel for lm-eval (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lm-eval: filename=lm_eval-0.4.0-py3-none-any.whl size=994996 sha256=60c3f8d28222773a61b9feb9049529bc5151594a979a647f879a4f7a7eba9eb2\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/b5/b3/4d97da1bde0b79da99e75ec0eb69f838d1fe02f83ab7a01bb4\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=6bfea36b618274e7522de49891e31c7b8b5bd33db072109cae15943df746e177\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16863 sha256=2175613c2113ace6ce702e463dff564c5401de41ec7a3ab1614a315918df86df\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n",
            "Successfully built lm-eval rouge-score sqlitedict\n",
            "Installing collected packages: texttable, sqlitedict, distlib, zstandard, virtualenv, tcolorpy, pyproject-api, pybind11, portalocker, pathvalidate, ordered-set, mbstrdecoder, jsonlines, coverage, colorama, blessings, typepy, tqdm-multiprocess, tox, sacrebleu, rouge-score, responses, deepdiff, colour-runner, codecov, rootpath, inspecta, DataProperty, tabledata, evaluate, attributedict, pytablewriter, lm-eval, autoawq\n",
            "Successfully installed DataProperty-1.0.1 attributedict-0.3.0 autoawq-0.1.8 blessings-1.7 codecov-2.1.13 colorama-0.4.6 colour-runner-0.1.1 coverage-7.4.0 deepdiff-6.7.1 distlib-0.3.8 evaluate-0.4.1 inspecta-0.1.3 jsonlines-4.0.0 lm-eval-0.4.0 mbstrdecoder-1.1.3 ordered-set-4.1.0 pathvalidate-3.2.0 portalocker-2.8.2 pybind11-2.11.1 pyproject-api-1.6.1 pytablewriter-1.2.0 responses-0.18.0 rootpath-0.1.1 rouge-score-0.1.2 sacrebleu-2.4.0 sqlitedict-2.1.0 tabledata-1.3.3 tcolorpy-0.1.4 texttable-1.7.0 tox-4.12.0 tqdm-multiprocess-0.0.11 typepy-1.3.2 virtualenv-20.25.0 zstandard-0.22.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "Successfully installed transformers-4.36.2\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install git+https://github.com/huggingface/optimum-benchmark.git\n",
        "!pip install bitsandbytes\n",
        "!pip install auto-gptq\n",
        "!pip install autoawq\n",
        "!pip install --upgrade transformers #Google Colab doesn't use by default the last version of Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the configuration for optimum-benchmark.\n",
        "\n",
        "Here we benchmark for inference, using different batch sizes, Mistral 7B loaded as fp16.\n",
        "If you run this notebook on Google Colab, you will need the A100 only for this part. The following benchmarks would run on the T4."
      ],
      "metadata": {
        "id": "UO6NNcn90ZNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "YAML_DEFAULT=\"\"\"\n",
        "defaults:\n",
        "  - backend: pytorch # default backend\n",
        "  - benchmark: inference # we will monitor the inference\n",
        "  - launcher: process\n",
        "  - experiment # inheriting from experiment config\n",
        "  - _self_ # for hydra 1.1 compatibility\n",
        "  - override hydra/job_logging: colorlog # colorful logging\n",
        "  - override hydra/hydra_logging: colorlog # colorful logging\n",
        "\n",
        "hydra:\n",
        "  run:\n",
        "    dir: experiments/${experiment_name} #The results will be reported in this directory. Note that \"experiment_name\" refers to the configuration field name \"experiment_name\" below\n",
        "  sweep:\n",
        "    dir: experiments/${experiment_name}\n",
        "  job:\n",
        "    chdir: true\n",
        "    env_set: #These are environment variable that you may want to set before running the benchmark\n",
        "      CUDA_VISIBLE_DEVICES: 0\n",
        "      CUDA_DEVICE_ORDER: PCI_BUS_ID\n",
        "  sweeper:\n",
        "    params:\n",
        "      benchmark.input_shapes.batch_size: 1,2,4,8,16 #we will try all these batch sizes\n",
        "\n",
        "experiment_name: fp16-batch_size(${benchmark.input_shapes.batch_size})-sequence_length(${benchmark.input_shapes.sequence_length})-new_tokens(${benchmark.new_tokens})\n",
        "model: mistralai/Mistral-7B-v0.1 #The model that we want to evaluate. It can be from the Hugging Face Hub or local directory\n",
        "device: cuda #Which device to use for the benchmark. We will use CUDA, i.e., the GPU\n",
        "\n",
        "backend:\n",
        "  torch_dtype: float16 #The model will be loaded with fp16\n",
        "\n",
        "benchmark:\n",
        "  memory: true #We will monitor the memory usage\n",
        "  warmup_runs: 10 #Before the monitoring starts, the inference will be run 10 times for warming up\n",
        "\n",
        "  new_tokens: 1000 #Inference will generate 1000 tokens\n",
        "  input_shapes:\n",
        "    sequence_length: 512 #Prompt will have 512 tokens\n",
        "\"\"\"\n",
        "\n",
        "with open(\"mistral_7b_ob.yaml\", 'w') as f:\n",
        "  f.write(YAML_DEFAULT)"
      ],
      "metadata": {
        "id": "a7YIfxXma5C2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!optimum-benchmark --config-dir ./ --config-name mistral_7b_ob --multirun"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSaGy-dIxbQd",
        "outputId": "babaf8a4-55a2-421b-eb77-871d49f4d0fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-16 04:32:34.977174: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-16 04:32:34.977223: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-16 04:32:34.978673: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-16 04:32:36.174801: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[\u001b[36m2024-01-16 04:32:39,634\u001b[0m][\u001b[35mHYDRA\u001b[0m] Launching 5 jobs locally\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:32:39,635\u001b[0m][\u001b[35mHYDRA\u001b[0m] \t#0 : benchmark.input_shapes.batch_size=1\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:32:39,799\u001b[0m][\u001b[34mlauncher\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring process launcher\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:32:39,799\u001b[0m][\u001b[34mprocess\u001b[0m][\u001b[32mINFO\u001b[0m] - Setting multiprocessing start method to spawn.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:32:39,802\u001b[0m][\u001b[34mprocess\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Launched worker process with PID 7640.\u001b[0m\n",
            "2024-01-16 04:32:42.595894: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-16 04:32:42.595947: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-16 04:32:42.597119: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-16 04:32:43.773120: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[\u001b[36m2024-01-16 04:32:46,825\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - PyTorch version 2.1.0+cu121 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:32:46,826\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - TensorFlow version 2.15.0 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:32:46,827\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - JAX version 0.4.23 available.\u001b[0m\n",
            "config.json: 100% 571/571 [00:00<00:00, 3.55MB/s]\n",
            "tokenizer_config.json: 100% 967/967 [00:00<00:00, 7.62MB/s]\n",
            "tokenizer.model: 100% 493k/493k [00:00<00:00, 11.7MB/s]\n",
            "tokenizer.json: 100% 1.80M/1.80M [00:00<00:00, 18.7MB/s]\n",
            "special_tokens_map.json: 100% 72.0/72.0 [00:00<00:00, 568kB/s]\n",
            "generation_config.json: 100% 116/116 [00:00<00:00, 678kB/s]\n",
            "[\u001b[36m2024-01-16 04:32:48,181\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:32:48,181\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Inferred class AutoModelForCausalLM for task text-generation and model_type mistral\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:32:48,181\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Disabling gradients\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:32:48,181\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading model with pretrained weights\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:32:48,181\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading model directly on device: cuda\u001b[0m\n",
            "model.safetensors.index.json: 100% 25.1k/25.1k [00:00<00:00, 101MB/s]\n",
            "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00001-of-00002.safetensors:   0% 0.00/9.94G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 21.0M/9.94G [00:00<00:56, 176MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 52.4M/9.94G [00:00<00:48, 204MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 83.9M/9.94G [00:00<00:46, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 115M/9.94G [00:00<00:45, 218MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 147M/9.94G [00:00<00:44, 221MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 178M/9.94G [00:00<00:44, 222MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 210M/9.94G [00:00<00:43, 223MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 241M/9.94G [00:01<00:43, 222MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 273M/9.94G [00:01<00:43, 222MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 304M/9.94G [00:01<00:43, 224MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 336M/9.94G [00:01<00:42, 226MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 367M/9.94G [00:01<00:42, 226MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 398M/9.94G [00:01<00:42, 226MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 430M/9.94G [00:01<00:42, 226MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 461M/9.94G [00:02<00:42, 225MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 493M/9.94G [00:02<00:42, 222MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 524M/9.94G [00:02<00:42, 223MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 556M/9.94G [00:02<00:41, 224MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 587M/9.94G [00:02<00:41, 224MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 619M/9.94G [00:02<00:41, 223MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 650M/9.94G [00:02<00:41, 223MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 682M/9.94G [00:03<00:41, 225MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 713M/9.94G [00:03<00:40, 226MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 744M/9.94G [00:03<00:40, 225MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 776M/9.94G [00:03<00:42, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 807M/9.94G [00:03<00:42, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 839M/9.94G [00:03<00:42, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 870M/9.94G [00:03<00:41, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 902M/9.94G [00:04<00:41, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 933M/9.94G [00:04<00:42, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 965M/9.94G [00:04<00:41, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 996M/9.94G [00:04<00:41, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 1.03G/9.94G [00:04<00:41, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 1.06G/9.94G [00:04<00:41, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 1.09G/9.94G [00:04<00:40, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 1.12G/9.94G [00:05<00:41, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 1.15G/9.94G [00:05<00:41, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 1.18G/9.94G [00:05<00:42, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 1.22G/9.94G [00:05<00:41, 208MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 1.24G/9.94G [00:05<00:41, 208MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 1.26G/9.94G [00:05<00:41, 208MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 1.28G/9.94G [00:05<00:41, 207MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 1.30G/9.94G [00:05<00:41, 207MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 1.33G/9.94G [00:06<00:40, 210MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 1.36G/9.94G [00:06<00:40, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 1.39G/9.94G [00:06<00:40, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 1.43G/9.94G [00:06<00:39, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 1.46G/9.94G [00:06<00:39, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 1.49G/9.94G [00:06<00:39, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 1.52G/9.94G [00:06<00:38, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 1.55G/9.94G [00:07<00:38, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 1.58G/9.94G [00:07<00:38, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 1.61G/9.94G [00:07<00:38, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 1.65G/9.94G [00:07<00:37, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 1.68G/9.94G [00:07<00:37, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 1.71G/9.94G [00:07<00:37, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 1.74G/9.94G [00:08<00:37, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 1.77G/9.94G [00:08<00:37, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 1.80G/9.94G [00:08<01:18, 104MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 1.84G/9.94G [00:08<01:06, 122MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 1.87G/9.94G [00:09<00:57, 141MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 1.90G/9.94G [00:09<00:50, 158MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 1.93G/9.94G [00:09<00:46, 174MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 1.96G/9.94G [00:09<00:42, 186MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 1.99G/9.94G [00:09<00:41, 194MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 2.02G/9.94G [00:09<00:39, 200MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 2.06G/9.94G [00:09<00:38, 205MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 2.09G/9.94G [00:10<00:38, 205MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 2.12G/9.94G [00:10<00:37, 208MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 2.15G/9.94G [00:10<00:36, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 2.18G/9.94G [00:10<00:36, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 2.21G/9.94G [00:10<00:36, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 2.24G/9.94G [00:10<00:35, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 2.28G/9.94G [00:10<00:35, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 2.31G/9.94G [00:11<00:36, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 2.34G/9.94G [00:11<00:35, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 2.37G/9.94G [00:11<00:35, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 2.40G/9.94G [00:11<00:35, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 2.43G/9.94G [00:11<00:34, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 2.46G/9.94G [00:11<00:34, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 2.50G/9.94G [00:12<00:34, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 2.53G/9.94G [00:12<00:49, 151MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 2.56G/9.94G [00:12<00:44, 166MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 2.59G/9.94G [00:12<00:41, 178MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 2.62G/9.94G [00:12<00:38, 189MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 2.65G/9.94G [00:12<00:36, 197MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 2.67G/9.94G [00:13<00:36, 200MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 2.71G/9.94G [00:13<00:35, 205MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 2.74G/9.94G [00:13<00:34, 211MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 2.77G/9.94G [00:13<00:33, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 2.80G/9.94G [00:13<00:34, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 2.82G/9.94G [00:13<00:34, 205MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 2.85G/9.94G [00:13<00:34, 208MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 2.88G/9.94G [00:14<00:33, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 2.92G/9.94G [00:14<00:32, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 2.95G/9.94G [00:14<00:32, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 2.98G/9.94G [00:14<00:32, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 3.01G/9.94G [00:14<00:31, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 3.04G/9.94G [00:14<00:31, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 3.07G/9.94G [00:14<00:32, 211MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 3.10G/9.94G [00:15<00:32, 210MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 3.14G/9.94G [00:15<00:32, 208MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 3.16G/9.94G [00:15<00:32, 208MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 3.19G/9.94G [00:15<00:32, 210MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 3.21G/9.94G [00:15<00:32, 209MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 3.23G/9.94G [00:15<00:32, 207MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 3.25G/9.94G [00:15<00:32, 205MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 3.27G/9.94G [00:15<00:32, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 3.30G/9.94G [00:16<00:32, 207MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 3.32G/9.94G [00:16<00:37, 176MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 3.34G/9.94G [00:16<00:38, 171MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 3.37G/9.94G [00:16<00:37, 176MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 3.39G/9.94G [00:16<00:36, 180MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 3.41G/9.94G [00:16<00:36, 181MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 3.43G/9.94G [00:16<00:35, 185MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 3.45G/9.94G [00:16<00:34, 191MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 3.47G/9.94G [00:16<00:33, 194MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 3.50G/9.94G [00:17<00:32, 200MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 3.52G/9.94G [00:17<00:31, 201MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 3.55G/9.94G [00:17<00:31, 205MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 3.59G/9.94G [00:17<00:30, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 3.62G/9.94G [00:17<00:30, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 3.64G/9.94G [00:17<00:31, 203MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 3.66G/9.94G [00:17<00:30, 204MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 3.68G/9.94G [00:18<00:30, 204MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 3.71G/9.94G [00:18<00:30, 207MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 3.73G/9.94G [00:18<00:30, 207MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 3.76G/9.94G [00:18<00:29, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 3.80G/9.94G [00:18<00:28, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 3.83G/9.94G [00:18<00:27, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 3.86G/9.94G [00:18<00:27, 222MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 3.89G/9.94G [00:18<00:27, 224MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 3.92G/9.94G [00:19<00:26, 225MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 3.95G/9.94G [00:19<00:26, 225MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 3.98G/9.94G [00:19<00:26, 224MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 4.02G/9.94G [00:19<00:26, 225MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 4.05G/9.94G [00:19<00:26, 223MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 4.08G/9.94G [00:19<00:26, 225MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 4.11G/9.94G [00:19<00:25, 225MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 4.14G/9.94G [00:20<00:25, 225MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 4.17G/9.94G [00:20<00:25, 225MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 4.20G/9.94G [00:20<00:25, 225MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 4.24G/9.94G [00:20<00:25, 224MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 4.27G/9.94G [00:20<00:25, 225MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 4.30G/9.94G [00:20<00:25, 223MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 4.33G/9.94G [00:20<00:25, 221MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 4.36G/9.94G [00:21<00:25, 223MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 4.39G/9.94G [00:21<00:25, 221MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 4.42G/9.94G [00:21<00:28, 192MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 4.45G/9.94G [00:21<00:28, 192MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 4.48G/9.94G [00:21<00:27, 202MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 4.51G/9.94G [00:21<00:25, 209MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 4.54G/9.94G [00:21<00:25, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 4.57G/9.94G [00:22<00:24, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 4.60G/9.94G [00:22<00:24, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 4.63G/9.94G [00:22<00:24, 221MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 4.67G/9.94G [00:22<00:23, 222MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 4.70G/9.94G [00:22<00:23, 222MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 4.73G/9.94G [00:22<00:23, 222MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 4.76G/9.94G [00:22<00:23, 221MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 4.79G/9.94G [00:23<00:23, 221MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 4.82G/9.94G [00:23<00:23, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 4.85G/9.94G [00:23<00:23, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 4.89G/9.94G [00:23<00:23, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 4.92G/9.94G [00:23<00:24, 207MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 4.95G/9.94G [00:23<00:23, 210MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 4.98G/9.94G [00:23<00:23, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 5.01G/9.94G [00:24<00:23, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 5.04G/9.94G [00:24<00:22, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 5.08G/9.94G [00:24<00:22, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 5.11G/9.94G [00:24<00:21, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 5.14G/9.94G [00:24<00:21, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 5.17G/9.94G [00:24<00:21, 221MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 5.20G/9.94G [00:25<00:31, 153MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 5.22G/9.94G [00:25<00:32, 147MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 5.25G/9.94G [00:25<00:28, 165MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 5.28G/9.94G [00:25<00:26, 179MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 5.32G/9.94G [00:25<00:24, 190MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 5.35G/9.94G [00:25<00:23, 197MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 5.38G/9.94G [00:26<00:22, 204MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 5.41G/9.94G [00:26<00:21, 209MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 5.44G/9.94G [00:26<00:21, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 5.47G/9.94G [00:26<00:20, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 5.51G/9.94G [00:26<00:20, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 5.54G/9.94G [00:26<00:20, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 5.57G/9.94G [00:26<00:20, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 5.60G/9.94G [00:27<00:20, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 5.63G/9.94G [00:27<00:19, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 5.66G/9.94G [00:27<00:19, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 5.69G/9.94G [00:27<00:19, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 5.73G/9.94G [00:27<00:19, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 5.76G/9.94G [00:27<00:19, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 5.79G/9.94G [00:27<00:19, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 5.82G/9.94G [00:28<00:18, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 5.85G/9.94G [00:28<00:18, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 5.88G/9.94G [00:28<00:19, 210MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 5.91G/9.94G [00:28<00:19, 209MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 5.95G/9.94G [00:28<00:18, 211MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 5.98G/9.94G [00:28<00:18, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 6.01G/9.94G [00:28<00:18, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 6.04G/9.94G [00:29<00:18, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 6.07G/9.94G [00:29<00:18, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 6.10G/9.94G [00:29<00:17, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 6.13G/9.94G [00:29<00:21, 181MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 6.16G/9.94G [00:29<00:20, 182MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 6.19G/9.94G [00:29<00:19, 191MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 6.21G/9.94G [00:30<00:19, 194MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 6.24G/9.94G [00:30<00:18, 205MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 6.27G/9.94G [00:30<00:17, 209MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 6.30G/9.94G [00:30<00:17, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 6.33G/9.94G [00:30<00:16, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 6.36G/9.94G [00:30<00:16, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 6.40G/9.94G [00:30<00:16, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 6.43G/9.94G [00:30<00:16, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 6.46G/9.94G [00:31<00:15, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 6.49G/9.94G [00:31<00:15, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 6.52G/9.94G [00:31<00:15, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 6.55G/9.94G [00:31<00:15, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 6.59G/9.94G [00:31<00:16, 200MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 6.62G/9.94G [00:31<00:16, 205MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 6.65G/9.94G [00:32<00:15, 209MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 6.68G/9.94G [00:32<00:15, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 6.71G/9.94G [00:32<00:14, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 6.74G/9.94G [00:32<00:14, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 6.77G/9.94G [00:32<00:14, 221MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 6.81G/9.94G [00:32<00:14, 221MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 6.84G/9.94G [00:32<00:14, 222MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 6.87G/9.94G [00:33<00:13, 222MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 6.90G/9.94G [00:33<00:13, 221MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 6.93G/9.94G [00:33<00:13, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 6.96G/9.94G [00:33<00:13, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 6.99G/9.94G [00:33<00:13, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 7.03G/9.94G [00:33<00:13, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 7.06G/9.94G [00:33<00:13, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 7.09G/9.94G [00:34<00:13, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 7.12G/9.94G [00:34<00:12, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 7.15G/9.94G [00:34<00:12, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 7.18G/9.94G [00:34<00:12, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 7.21G/9.94G [00:34<00:12, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 7.25G/9.94G [00:34<00:12, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 7.28G/9.94G [00:34<00:12, 211MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 7.31G/9.94G [00:35<00:12, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 7.34G/9.94G [00:35<00:12, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 7.37G/9.94G [00:35<00:11, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 7.40G/9.94G [00:35<00:11, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 7.43G/9.94G [00:35<00:11, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 7.47G/9.94G [00:35<00:11, 221MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 7.50G/9.94G [00:35<00:11, 222MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 7.53G/9.94G [00:36<00:10, 221MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 7.56G/9.94G [00:36<00:10, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 7.59G/9.94G [00:36<00:10, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 7.62G/9.94G [00:36<00:10, 221MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 7.65G/9.94G [00:36<00:10, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 7.69G/9.94G [00:36<00:10, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 7.72G/9.94G [00:36<00:10, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 7.75G/9.94G [00:37<00:09, 223MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 7.78G/9.94G [00:37<00:09, 225MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 7.81G/9.94G [00:37<00:09, 222MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 7.84G/9.94G [00:37<00:09, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 7.87G/9.94G [00:37<00:09, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 7.91G/9.94G [00:37<00:09, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 7.94G/9.94G [00:37<00:09, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 7.97G/9.94G [00:38<00:09, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 8.00G/9.94G [00:38<00:09, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 8.03G/9.94G [00:38<00:09, 209MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 8.05G/9.94G [00:38<00:09, 208MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 8.07G/9.94G [00:38<00:08, 208MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 8.10G/9.94G [00:38<00:08, 207MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 8.13G/9.94G [00:38<00:08, 210MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 8.16G/9.94G [00:38<00:08, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 8.19G/9.94G [00:39<00:08, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 8.22G/9.94G [00:39<00:07, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 8.25G/9.94G [00:39<00:07, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 8.28G/9.94G [00:39<00:07, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 8.32G/9.94G [00:39<00:07, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 8.35G/9.94G [00:39<00:07, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 8.38G/9.94G [00:39<00:07, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 8.41G/9.94G [00:40<00:06, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 8.44G/9.94G [00:40<00:06, 221MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 8.47G/9.94G [00:40<00:06, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 8.50G/9.94G [00:40<00:06, 221MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 8.54G/9.94G [00:40<00:06, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 8.57G/9.94G [00:40<00:06, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 8.60G/9.94G [00:40<00:06, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 8.63G/9.94G [00:41<00:06, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 8.66G/9.94G [00:41<00:05, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 8.69G/9.94G [00:41<00:05, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 8.72G/9.94G [00:41<00:05, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 8.76G/9.94G [00:41<00:05, 221MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 8.79G/9.94G [00:41<00:05, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 8.82G/9.94G [00:42<00:05, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 8.85G/9.94G [00:42<00:05, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 8.88G/9.94G [00:42<00:04, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 8.91G/9.94G [00:42<00:04, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 8.94G/9.94G [00:42<00:04, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 8.98G/9.94G [00:42<00:04, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 9.01G/9.94G [00:42<00:04, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 9.04G/9.94G [00:43<00:04, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 9.07G/9.94G [00:43<00:04, 211MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 9.10G/9.94G [00:43<00:04, 205MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 9.12G/9.94G [00:43<00:04, 204MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 9.14G/9.94G [00:43<00:03, 202MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 9.18G/9.94G [00:43<00:03, 205MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 9.20G/9.94G [00:43<00:03, 205MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 9.23G/9.94G [00:43<00:03, 211MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 9.26G/9.94G [00:44<00:03, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 9.29G/9.94G [00:44<00:03, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 9.32G/9.94G [00:44<00:02, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 9.35G/9.94G [00:44<00:02, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 9.38G/9.94G [00:44<00:02, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 9.42G/9.94G [00:44<00:02, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 9.45G/9.94G [00:45<00:03, 149MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 9.48G/9.94G [00:45<00:02, 165MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 9.51G/9.94G [00:45<00:02, 180MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 9.54G/9.94G [00:45<00:02, 189MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 9.57G/9.94G [00:45<00:01, 195MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 9.60G/9.94G [00:45<00:01, 201MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 9.64G/9.94G [00:46<00:01, 205MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 9.67G/9.94G [00:46<00:01, 207MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 9.70G/9.94G [00:46<00:01, 209MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 9.73G/9.94G [00:46<00:01, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 9.76G/9.94G [00:46<00:00, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 9.79G/9.94G [00:46<00:00, 185MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 9.83G/9.94G [00:46<00:00, 193MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 9.86G/9.94G [00:47<00:00, 199MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 9.88G/9.94G [00:47<00:00, 199MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 9.90G/9.94G [00:47<00:00, 201MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 9.94G/9.94G [00:47<00:00, 209MB/s]\n",
            "Downloading shards:  50% 1/2 [00:47<00:47, 47.62s/it]\n",
            "model-00002-of-00002.safetensors:   0% 0.00/4.54G [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   0% 21.0M/4.54G [00:00<00:22, 197MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   1% 52.4M/4.54G [00:00<00:21, 214MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 83.9M/4.54G [00:00<00:20, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   3% 115M/4.54G [00:00<00:20, 219MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:   3% 147M/4.54G [00:00<00:20, 213MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 178M/4.54G [00:00<00:20, 216MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 210M/4.54G [00:00<00:19, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 241M/4.54G [00:01<00:19, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   6% 273M/4.54G [00:01<00:19, 215MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   7% 304M/4.54G [00:01<00:19, 216MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   7% 336M/4.54G [00:01<00:19, 216MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   8% 367M/4.54G [00:01<00:19, 215MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   9% 398M/4.54G [00:01<00:19, 215MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   9% 430M/4.54G [00:01<00:19, 215MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  10% 461M/4.54G [00:02<00:18, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  11% 493M/4.54G [00:02<00:18, 219MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  12% 524M/4.54G [00:02<00:18, 220MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  12% 556M/4.54G [00:02<00:17, 222MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  13% 587M/4.54G [00:02<00:17, 220MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  14% 619M/4.54G [00:02<00:17, 219MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  14% 650M/4.54G [00:02<00:17, 220MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  15% 682M/4.54G [00:03<00:17, 221MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 713M/4.54G [00:03<00:17, 224MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 744M/4.54G [00:03<00:16, 224MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  17% 776M/4.54G [00:03<00:16, 224MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  18% 807M/4.54G [00:03<00:16, 220MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  18% 839M/4.54G [00:03<00:16, 222MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  19% 870M/4.54G [00:03<00:16, 223MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  20% 902M/4.54G [00:04<00:16, 221MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  21% 933M/4.54G [00:04<00:16, 219MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  21% 965M/4.54G [00:04<00:16, 216MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  22% 996M/4.54G [00:04<00:16, 215MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  23% 1.03G/4.54G [00:04<00:16, 213MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  23% 1.06G/4.54G [00:04<00:16, 215MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  24% 1.09G/4.54G [00:05<00:16, 215MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  25% 1.12G/4.54G [00:05<00:15, 216MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  25% 1.15G/4.54G [00:05<00:15, 216MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  26% 1.18G/4.54G [00:05<00:15, 218MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  27% 1.22G/4.54G [00:05<00:15, 219MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  27% 1.25G/4.54G [00:05<00:14, 221MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  28% 1.28G/4.54G [00:05<00:14, 223MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  29% 1.31G/4.54G [00:06<00:14, 223MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  30% 1.34G/4.54G [00:06<00:14, 220MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  30% 1.37G/4.54G [00:06<00:15, 200MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  31% 1.41G/4.54G [00:06<00:15, 205MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  32% 1.44G/4.54G [00:06<00:14, 208MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  32% 1.47G/4.54G [00:06<00:14, 210MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  33% 1.50G/4.54G [00:06<00:14, 211MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  34% 1.53G/4.54G [00:07<00:14, 215MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  34% 1.56G/4.54G [00:07<00:13, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  35% 1.59G/4.54G [00:07<00:13, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  36% 1.63G/4.54G [00:07<00:13, 215MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  36% 1.66G/4.54G [00:07<00:13, 216MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 1.69G/4.54G [00:07<00:13, 214MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  38% 1.72G/4.54G [00:07<00:13, 215MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  39% 1.75G/4.54G [00:08<00:12, 216MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  39% 1.78G/4.54G [00:08<00:12, 216MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  40% 1.81G/4.54G [00:08<00:12, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  41% 1.85G/4.54G [00:08<00:12, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  41% 1.88G/4.54G [00:08<00:12, 218MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 1.91G/4.54G [00:08<00:12, 219MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  43% 1.94G/4.54G [00:08<00:11, 218MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  43% 1.97G/4.54G [00:09<00:11, 218MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  44% 2.00G/4.54G [00:09<00:12, 209MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  45% 2.03G/4.54G [00:09<00:12, 207MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  45% 2.06G/4.54G [00:09<00:11, 208MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  46% 2.09G/4.54G [00:09<00:11, 209MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  47% 2.12G/4.54G [00:09<00:11, 210MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  47% 2.15G/4.54G [00:09<00:11, 209MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  48% 2.17G/4.54G [00:10<00:11, 208MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  48% 2.19G/4.54G [00:10<00:11, 207MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  49% 2.21G/4.54G [00:10<00:11, 207MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  49% 2.23G/4.54G [00:10<00:11, 208MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  50% 2.26G/4.54G [00:10<00:10, 210MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 2.30G/4.54G [00:10<00:10, 211MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 2.33G/4.54G [00:10<00:10, 213MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  52% 2.36G/4.54G [00:10<00:10, 212MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  53% 2.39G/4.54G [00:11<00:10, 212MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  53% 2.42G/4.54G [00:11<00:09, 216MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  54% 2.45G/4.54G [00:11<00:09, 219MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  55% 2.49G/4.54G [00:11<00:09, 220MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  55% 2.52G/4.54G [00:11<00:09, 219MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  56% 2.55G/4.54G [00:11<00:09, 219MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  57% 2.58G/4.54G [00:11<00:08, 220MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  58% 2.61G/4.54G [00:12<00:08, 220MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  58% 2.64G/4.54G [00:12<00:08, 220MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  59% 2.67G/4.54G [00:12<00:08, 219MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 2.71G/4.54G [00:12<00:08, 219MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 2.74G/4.54G [00:12<00:08, 220MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  61% 2.77G/4.54G [00:12<00:08, 207MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  62% 2.80G/4.54G [00:12<00:08, 213MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  62% 2.83G/4.54G [00:13<00:07, 215MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  63% 2.86G/4.54G [00:13<00:07, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  64% 2.89G/4.54G [00:13<00:07, 218MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  64% 2.93G/4.54G [00:13<00:07, 218MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  65% 2.96G/4.54G [00:13<00:07, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  66% 2.99G/4.54G [00:13<00:07, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  67% 3.02G/4.54G [00:13<00:06, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  67% 3.05G/4.54G [00:14<00:06, 216MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  68% 3.08G/4.54G [00:14<00:06, 218MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  69% 3.11G/4.54G [00:14<00:06, 220MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  69% 3.15G/4.54G [00:14<00:06, 220MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  70% 3.18G/4.54G [00:14<00:06, 219MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 3.21G/4.54G [00:14<00:06, 221MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 3.24G/4.54G [00:14<00:05, 220MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  72% 3.27G/4.54G [00:15<00:05, 219MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  73% 3.30G/4.54G [00:15<00:05, 222MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  73% 3.33G/4.54G [00:15<00:05, 222MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  74% 3.37G/4.54G [00:15<00:05, 220MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  75% 3.40G/4.54G [00:15<00:05, 219MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  76% 3.43G/4.54G [00:15<00:05, 215MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  76% 3.46G/4.54G [00:16<00:05, 213MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  77% 3.49G/4.54G [00:16<00:05, 208MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  78% 3.52G/4.54G [00:16<00:04, 212MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  78% 3.55G/4.54G [00:16<00:06, 150MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  79% 3.59G/4.54G [00:16<00:05, 165MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  80% 3.62G/4.54G [00:16<00:05, 178MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  80% 3.65G/4.54G [00:17<00:04, 190MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  81% 3.68G/4.54G [00:17<00:04, 197MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  82% 3.71G/4.54G [00:17<00:04, 203MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  82% 3.74G/4.54G [00:17<00:03, 207MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  83% 3.77G/4.54G [00:17<00:03, 209MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  84% 3.81G/4.54G [00:17<00:03, 211MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  85% 3.84G/4.54G [00:17<00:03, 212MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  85% 3.87G/4.54G [00:18<00:03, 213MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  86% 3.90G/4.54G [00:18<00:02, 215MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  87% 3.93G/4.54G [00:18<00:02, 218MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  87% 3.96G/4.54G [00:18<00:02, 219MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  88% 4.00G/4.54G [00:18<00:02, 220MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  89% 4.03G/4.54G [00:18<00:02, 220MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  89% 4.06G/4.54G [00:18<00:02, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  90% 4.09G/4.54G [00:19<00:02, 218MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  91% 4.12G/4.54G [00:19<00:01, 213MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  91% 4.15G/4.54G [00:19<00:01, 216MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  92% 4.18G/4.54G [00:19<00:01, 218MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  93% 4.22G/4.54G [00:19<00:01, 220MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  94% 4.25G/4.54G [00:19<00:01, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  94% 4.28G/4.54G [00:19<00:01, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  95% 4.31G/4.54G [00:20<00:01, 215MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  96% 4.34G/4.54G [00:20<00:00, 215MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  96% 4.37G/4.54G [00:20<00:00, 215MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  97% 4.40G/4.54G [00:20<00:00, 216MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  98% 4.44G/4.54G [00:20<00:00, 216MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  98% 4.47G/4.54G [00:20<00:00, 209MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  99% 4.50G/4.54G [00:21<00:00, 210MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 4.54G/4.54G [00:21<00:00, 214MB/s]\n",
            "Downloading shards: 100% 2/2 [01:08<00:00, 34.47s/it]\n",
            "Loading checkpoint shards: 100% 2/2 [00:04<00:00,  2.32s/it]\n",
            "[\u001b[36m2024-01-16 04:34:02,528\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Turning on model's eval mode\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:34:02,532\u001b[0m][\u001b[34mbenchmark\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:34:02,533\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Running inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:34:02,533\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Updating input shapes with model shapes\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:34:02,533\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing backend for inference\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:34:02,533\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating input generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:34:02,533\u001b[0m][\u001b[34minput-generator\u001b[0m][\u001b[32mINFO\u001b[0m] - Using text-generation task generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:34:02,534\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:34:02,535\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:34:02,535\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:34:02,535\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:34:02,535\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:34:03,462\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory used: 16394 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:34:03,462\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory reserved: 15386 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:34:03,462\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory allocated: 15199 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:34:03,463\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:34:03,463\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:34:03,463\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:34:03,463\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:34:03,463\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:34:42,180\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory used: 16623 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:34:42,181\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory reserved: 15613 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:34:42,181\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory allocated: 15452 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:34:42,181\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:34:42,181\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:34:42,182\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:34:42,669\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:34:52,761\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass latency: 5.29e-02 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:34:52,761\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass throughput: 18.90 (samples/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:34:52,762\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:34:52,762\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:34:52,762\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:35:31,267\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:36:09,853\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass latency: 3.86e+01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:36:09,853\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass throughput: 25.90 (tokens/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:36:09,854\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving results\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:36:09,859\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Cleaning pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:36:09,859\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Deleting pretrained model\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:36:10,302\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Emptying CUDA cache\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:36:11,884\u001b[0m][\u001b[35mHYDRA\u001b[0m] \t#1 : benchmark.input_shapes.batch_size=2\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:36:12,051\u001b[0m][\u001b[34mlauncher\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring process launcher\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:36:12,053\u001b[0m][\u001b[34mprocess\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Launched worker process with PID 8541.\u001b[0m\n",
            "2024-01-16 04:36:14.835105: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-16 04:36:14.835160: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-16 04:36:14.836560: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-16 04:36:16.045732: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[\u001b[36m2024-01-16 04:36:19,085\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - PyTorch version 2.1.0+cu121 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:36:19,086\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - TensorFlow version 2.15.0 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:36:19,087\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - JAX version 0.4.23 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:36:19,589\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:36:19,590\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Inferred class AutoModelForCausalLM for task text-generation and model_type mistral\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:36:19,590\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Disabling gradients\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:36:19,590\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading model with pretrained weights\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:36:19,590\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading model directly on device: cuda\u001b[0m\n",
            "Loading checkpoint shards: 100% 2/2 [00:05<00:00,  2.69s/it]\n",
            "[\u001b[36m2024-01-16 04:36:25,447\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Turning on model's eval mode\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:36:25,452\u001b[0m][\u001b[34mbenchmark\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:36:25,452\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Running inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:36:25,453\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Updating input shapes with model shapes\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:36:25,453\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing backend for inference\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:36:25,453\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating input generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:36:25,453\u001b[0m][\u001b[34minput-generator\u001b[0m][\u001b[32mINFO\u001b[0m] - Using text-generation task generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:36:25,454\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:36:25,454\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:36:25,454\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:36:25,455\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:36:25,455\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:36:26,287\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory used: 16514 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:36:26,287\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory reserved: 15506 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:36:26,287\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory allocated: 15369 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:36:26,288\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:36:26,288\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:36:26,340\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:36:26,341\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:36:26,341\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:37:06,500\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory used: 17155 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:37:06,501\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory reserved: 16145 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:37:06,501\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory allocated: 15877 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:37:06,501\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:37:06,502\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:37:06,502\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:37:07,418\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:37:17,518\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass latency: 9.84e-02 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:37:17,519\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass throughput: 20.30 (samples/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:37:17,519\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:37:17,520\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:37:17,520\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:37:57,419\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:38:37,349\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass latency: 3.99e+01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:38:37,349\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass throughput: 50.10 (tokens/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:38:37,350\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving results\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:38:37,355\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Cleaning pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:38:37,355\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Deleting pretrained model\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:38:37,825\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Emptying CUDA cache\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:38:39,435\u001b[0m][\u001b[35mHYDRA\u001b[0m] \t#2 : benchmark.input_shapes.batch_size=4\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:38:39,601\u001b[0m][\u001b[34mlauncher\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring process launcher\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:38:39,602\u001b[0m][\u001b[34mprocess\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Launched worker process with PID 9179.\u001b[0m\n",
            "2024-01-16 04:38:42.387134: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-16 04:38:42.387181: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-16 04:38:42.388493: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-16 04:38:43.550163: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[\u001b[36m2024-01-16 04:38:46,599\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - PyTorch version 2.1.0+cu121 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:38:46,600\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - TensorFlow version 2.15.0 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:38:46,601\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - JAX version 0.4.23 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:38:47,096\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:38:47,096\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Inferred class AutoModelForCausalLM for task text-generation and model_type mistral\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:38:47,096\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Disabling gradients\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:38:47,096\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading model with pretrained weights\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:38:47,096\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading model directly on device: cuda\u001b[0m\n",
            "Loading checkpoint shards: 100% 2/2 [00:04<00:00,  2.32s/it]\n",
            "[\u001b[36m2024-01-16 04:38:52,200\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Turning on model's eval mode\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:38:52,205\u001b[0m][\u001b[34mbenchmark\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:38:52,205\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Running inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:38:52,205\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Updating input shapes with model shapes\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:38:52,206\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing backend for inference\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:38:52,206\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating input generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:38:52,206\u001b[0m][\u001b[34minput-generator\u001b[0m][\u001b[32mINFO\u001b[0m] - Using text-generation task generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:38:52,207\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:38:52,207\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:38:52,207\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:38:52,208\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:38:52,208\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:38:53,146\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory used: 17093 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:38:53,146\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory reserved: 16083 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:38:53,146\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory allocated: 15707 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:38:53,146\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:38:53,147\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:38:53,255\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:38:53,255\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:38:53,255\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:39:33,612\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory used: 18191 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:39:33,613\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory reserved: 17181 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:39:33,613\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory allocated: 16717 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:39:33,613\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:39:33,613\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:39:33,614\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:39:35,369\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:39:45,633\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass latency: 1.88e-01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:39:45,633\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass throughput: 21.30 (samples/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:39:45,634\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:39:45,634\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:39:45,634\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:40:25,653\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:41:05,954\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass latency: 4.03e+01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:41:05,955\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass throughput: 99.30 (tokens/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:41:05,955\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving results\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:41:05,960\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Cleaning pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:41:05,960\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Deleting pretrained model\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:41:06,442\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Emptying CUDA cache\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:41:08,116\u001b[0m][\u001b[35mHYDRA\u001b[0m] \t#3 : benchmark.input_shapes.batch_size=8\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:41:08,281\u001b[0m][\u001b[34mlauncher\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring process launcher\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:41:08,283\u001b[0m][\u001b[34mprocess\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Launched worker process with PID 9823.\u001b[0m\n",
            "2024-01-16 04:41:11.105604: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-16 04:41:11.105653: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-16 04:41:11.107376: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-16 04:41:12.321021: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[\u001b[36m2024-01-16 04:41:15,409\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - PyTorch version 2.1.0+cu121 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:41:15,409\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - TensorFlow version 2.15.0 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:41:15,411\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - JAX version 0.4.23 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:41:15,912\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:41:15,912\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Inferred class AutoModelForCausalLM for task text-generation and model_type mistral\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:41:15,912\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Disabling gradients\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:41:15,912\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading model with pretrained weights\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:41:15,912\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading model directly on device: cuda\u001b[0m\n",
            "Loading checkpoint shards: 100% 2/2 [00:04<00:00,  2.32s/it]\n",
            "[\u001b[36m2024-01-16 04:41:21,015\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Turning on model's eval mode\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:41:21,020\u001b[0m][\u001b[34mbenchmark\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:41:21,020\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Running inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:41:21,021\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Updating input shapes with model shapes\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:41:21,021\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing backend for inference\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:41:21,021\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating input generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:41:21,021\u001b[0m][\u001b[34minput-generator\u001b[0m][\u001b[32mINFO\u001b[0m] - Using text-generation task generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:41:21,022\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:41:21,023\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:41:21,023\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:41:21,023\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:41:21,023\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:41:22,278\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory used: 18057 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:41:22,278\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory reserved: 17049 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:41:22,278\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory allocated: 16385 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:41:22,278\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:41:22,279\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:41:22,490\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:41:22,490\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:41:22,491\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:42:03,087\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory used: 20238 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:42:03,088\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory reserved: 19228 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:42:03,088\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory allocated: 18401 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:42:03,088\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:42:03,089\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:42:03,089\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:42:06,509\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:42:16,935\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass latency: 3.65e-01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:42:16,936\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass throughput: 21.90 (samples/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:42:16,937\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:42:16,937\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:42:16,937\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:42:57,349\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:43:37,918\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass latency: 4.06e+01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:43:37,919\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass throughput: 197.00 (tokens/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:43:37,919\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving results\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:43:37,924\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Cleaning pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:43:37,925\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Deleting pretrained model\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:43:38,383\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Emptying CUDA cache\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:43:40,001\u001b[0m][\u001b[35mHYDRA\u001b[0m] \t#4 : benchmark.input_shapes.batch_size=16\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:43:40,179\u001b[0m][\u001b[34mlauncher\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring process launcher\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:43:40,180\u001b[0m][\u001b[34mprocess\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Launched worker process with PID 10483.\u001b[0m\n",
            "2024-01-16 04:43:42.974719: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-16 04:43:42.974775: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-16 04:43:42.976404: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-16 04:43:44.175999: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[\u001b[36m2024-01-16 04:43:47,271\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - PyTorch version 2.1.0+cu121 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:43:47,272\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - TensorFlow version 2.15.0 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:43:47,273\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - JAX version 0.4.23 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:43:47,814\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:43:47,814\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Inferred class AutoModelForCausalLM for task text-generation and model_type mistral\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:43:47,814\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Disabling gradients\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:43:47,814\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading model with pretrained weights\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:43:47,814\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading model directly on device: cuda\u001b[0m\n",
            "Loading checkpoint shards: 100% 2/2 [00:04<00:00,  2.30s/it]\n",
            "[\u001b[36m2024-01-16 04:43:52,893\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Turning on model's eval mode\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:43:52,898\u001b[0m][\u001b[34mbenchmark\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:43:52,898\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Running inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:43:52,898\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Updating input shapes with model shapes\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:43:52,898\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing backend for inference\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:43:52,899\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating input generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:43:52,899\u001b[0m][\u001b[34minput-generator\u001b[0m][\u001b[32mINFO\u001b[0m] - Using text-generation task generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:43:52,900\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:43:52,900\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:43:52,901\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:43:52,901\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:43:52,901\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:43:53,980\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory used: 19947 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:43:53,980\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory reserved: 18937 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:43:53,981\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory allocated: 17742 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:43:53,981\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:43:53,982\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:43:54,416\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:43:54,416\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:43:54,416\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:44:46,265\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory used: 24221 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:44:46,266\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory reserved: 23211 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:44:46,266\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory allocated: 21772 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:44:46,267\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:44:46,267\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:44:46,267\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:44:53,020\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:45:03,557\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass latency: 7.21e-01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:45:03,558\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass throughput: 22.20 (samples/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:45:03,559\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:45:03,559\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:45:03,559\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:45:55,053\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:46:46,599\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass latency: 5.15e+01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:46:46,600\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass throughput: 311.00 (tokens/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:46:46,600\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving results\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:46:46,605\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Cleaning pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:46:46,605\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Deleting pretrained model\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:46:47,058\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Emptying CUDA cache\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Benchmarking BNB's NF4 quantization without double quantization"
      ],
      "metadata": {
        "id": "H5KxXzRGLLrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "YAML_DEFAULT=\"\"\"\n",
        "defaults:\n",
        "  - backend: pytorch # default backend\n",
        "  - benchmark: inference # we will monitor the inference\n",
        "  - launcher: process\n",
        "  - experiment # inheriting from experiment config\n",
        "  - _self_ # for hydra 1.1 compatibility\n",
        "  - override hydra/job_logging: colorlog # colorful logging\n",
        "  - override hydra/hydra_logging: colorlog # colorful logging\n",
        "\n",
        "hydra:\n",
        "  run:\n",
        "    dir: experiments/${experiment_name} #The results will be reported in this directory. Note that \"experiment_name\" refers to the configuration field name \"experiment_name\" below\n",
        "  sweep:\n",
        "    dir: experiments/${experiment_name}\n",
        "  job:\n",
        "    chdir: true\n",
        "    env_set: #These are environment variable that you may want to set before running the benchmark\n",
        "      CUDA_VISIBLE_DEVICES: 0\n",
        "      CUDA_DEVICE_ORDER: PCI_BUS_ID\n",
        "  sweeper:\n",
        "    params:\n",
        "      benchmark.input_shapes.batch_size: 1,2,4,8,16 #we will try all these batch sizes\n",
        "\n",
        "experiment_name: bnb-batch_size(${benchmark.input_shapes.batch_size})-sequence_length(${benchmark.input_shapes.sequence_length})-new_tokens(${benchmark.new_tokens})\n",
        "model: mistralai/Mistral-7B-v0.1 #The model that we want to evaluate. It can be from the Hugging Face Hub or local directory\n",
        "device: cuda #Which device to use for the benchmark. We will use CUDA, i.e., the GPU\n",
        "\n",
        "backend:\n",
        "  torch_dtype: float16 #The model will be loaded with fp16\n",
        "  quantization_scheme: bnb\n",
        "  quantization_config:\n",
        "    load_in_4bit: true\n",
        "    bnb_4bit_compute_dtype: float16\n",
        "\n",
        "benchmark:\n",
        "  memory: true #We will monitor the memory usage\n",
        "  warmup_runs: 10 #Before the monitoring starts, the inference will be run 10 times for warming up\n",
        "\n",
        "  new_tokens: 1000 #Inference will generate 1000 tokens\n",
        "  input_shapes:\n",
        "    sequence_length: 512 #Prompt will have 512 tokens\n",
        "\"\"\"\n",
        "\n",
        "with open(\"mistral_7b_bnb_ob.yaml\", 'w') as f:\n",
        "  f.write(YAML_DEFAULT)"
      ],
      "metadata": {
        "id": "Yp_nlf1gLPRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!optimum-benchmark --config-dir ./ --config-name mistral_7b_bnb_ob --multirun"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uF6dswzMM0Ee",
        "outputId": "ba40f118-d406-43d9-c18a-4a7cbea480da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-16 04:46:52.647047: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-16 04:46:52.647091: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-16 04:46:52.648589: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-16 04:46:53.830690: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[\u001b[36m2024-01-16 04:46:57,283\u001b[0m][\u001b[35mHYDRA\u001b[0m] Launching 5 jobs locally\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:46:57,283\u001b[0m][\u001b[35mHYDRA\u001b[0m] \t#0 : benchmark.input_shapes.batch_size=1\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:46:57,450\u001b[0m][\u001b[34mlauncher\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring process launcher\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:46:57,451\u001b[0m][\u001b[34mprocess\u001b[0m][\u001b[32mINFO\u001b[0m] - Setting multiprocessing start method to spawn.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:46:57,453\u001b[0m][\u001b[34mprocess\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Launched worker process with PID 11357.\u001b[0m\n",
            "2024-01-16 04:47:00.200642: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-16 04:47:00.200690: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-16 04:47:00.201874: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-16 04:47:01.360290: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[\u001b[36m2024-01-16 04:47:04,429\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - PyTorch version 2.1.0+cu121 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:47:04,429\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - TensorFlow version 2.15.0 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:47:04,430\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - JAX version 0.4.23 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:47:04,985\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:47:04,985\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Inferred class AutoModelForCausalLM for task text-generation and model_type mistral\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:47:04,985\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Disabling gradients\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:47:04,985\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing quantization config\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:47:04,985\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing BitsAndBytes config\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:47:04,986\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading model with pretrained weights\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:47:04,986\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading quantized model\u001b[0m\n",
            "Loading checkpoint shards: 100% 2/2 [00:07<00:00,  3.67s/it]\n",
            "[\u001b[36m2024-01-16 04:47:14,396\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Turning on model's eval mode\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:47:14,401\u001b[0m][\u001b[34mbenchmark\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:47:14,402\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Running inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:47:14,402\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Updating input shapes with model shapes\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:47:14,402\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing backend for inference\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:47:14,402\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating input generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:47:14,402\u001b[0m][\u001b[34minput-generator\u001b[0m][\u001b[32mINFO\u001b[0m] - Using text-generation task generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:47:14,403\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:47:14,403\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:47:14,403\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:47:14,404\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:47:14,404\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:47:15,287\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory used: 6470 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:47:15,288\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory reserved: 5463 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:47:15,288\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory allocated: 5272 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:47:15,288\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:47:15,288\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:47:15,289\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:47:15,289\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:47:15,289\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:48:13,709\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory used: 6632 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:48:13,709\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory reserved: 5622 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:48:13,709\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory allocated: 5465 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:48:13,710\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:48:13,710\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:48:13,710\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:48:14,504\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:48:24,568\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass latency: 7.92e-02 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:48:24,568\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass throughput: 12.60 (samples/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:48:24,569\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:48:24,569\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:48:24,569\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:49:22,955\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:50:21,227\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass latency: 5.83e+01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:50:21,228\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass throughput: 17.20 (tokens/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:50:21,228\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving results\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:50:21,233\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Cleaning pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:50:21,234\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Deleting pretrained model\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:50:21,679\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Emptying CUDA cache\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:50:23,351\u001b[0m][\u001b[35mHYDRA\u001b[0m] \t#1 : benchmark.input_shapes.batch_size=2\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:50:23,518\u001b[0m][\u001b[34mlauncher\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring process launcher\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:50:23,520\u001b[0m][\u001b[34mprocess\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Launched worker process with PID 12251.\u001b[0m\n",
            "2024-01-16 04:50:26.284909: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-16 04:50:26.284958: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-16 04:50:26.286336: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-16 04:50:27.464206: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[\u001b[36m2024-01-16 04:50:30,538\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - PyTorch version 2.1.0+cu121 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:50:30,539\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - TensorFlow version 2.15.0 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:50:30,540\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - JAX version 0.4.23 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:50:31,049\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:50:31,049\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Inferred class AutoModelForCausalLM for task text-generation and model_type mistral\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:50:31,050\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Disabling gradients\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:50:31,050\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing quantization config\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:50:31,050\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing BitsAndBytes config\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:50:31,051\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading model with pretrained weights\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:50:31,051\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading quantized model\u001b[0m\n",
            "Loading checkpoint shards: 100% 2/2 [00:07<00:00,  3.53s/it]\n",
            "[\u001b[36m2024-01-16 04:50:40,189\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Turning on model's eval mode\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:50:40,194\u001b[0m][\u001b[34mbenchmark\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:50:40,195\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Running inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:50:40,195\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Updating input shapes with model shapes\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:50:40,195\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing backend for inference\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:50:40,195\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating input generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:50:40,195\u001b[0m][\u001b[34minput-generator\u001b[0m][\u001b[32mINFO\u001b[0m] - Using text-generation task generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:50:40,196\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:50:40,196\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:50:40,196\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:50:40,196\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:50:40,196\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:50:41,153\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory used: 6649 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:50:41,154\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory reserved: 5641 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:50:41,154\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory allocated: 5386 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:50:41,154\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:50:41,154\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:50:41,211\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:50:41,211\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:50:41,211\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:52:00,108\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory used: 7221 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:52:00,108\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory reserved: 6211 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:52:00,108\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory allocated: 5951 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:52:00,108\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:52:00,109\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:52:00,109\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:52:01,222\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:52:11,394\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass latency: 1.19e-01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:52:11,395\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass throughput: 16.80 (samples/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:52:11,395\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:52:11,395\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:52:11,396\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:53:30,323\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:54:48,739\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass latency: 7.84e+01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:54:48,739\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass throughput: 25.50 (tokens/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:54:48,740\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving results\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:54:48,745\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Cleaning pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:54:48,745\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Deleting pretrained model\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:54:49,220\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Emptying CUDA cache\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:54:50,921\u001b[0m][\u001b[35mHYDRA\u001b[0m] \t#2 : benchmark.input_shapes.batch_size=4\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:54:51,092\u001b[0m][\u001b[34mlauncher\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring process launcher\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:54:51,093\u001b[0m][\u001b[34mprocess\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Launched worker process with PID 13385.\u001b[0m\n",
            "2024-01-16 04:54:53.897328: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-16 04:54:53.897376: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-16 04:54:53.898734: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-16 04:54:55.081701: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[\u001b[36m2024-01-16 04:54:58,156\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - PyTorch version 2.1.0+cu121 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:54:58,157\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - TensorFlow version 2.15.0 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:54:58,158\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - JAX version 0.4.23 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:54:58,675\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:54:58,676\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Inferred class AutoModelForCausalLM for task text-generation and model_type mistral\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:54:58,676\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Disabling gradients\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:54:58,676\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing quantization config\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:54:58,676\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing BitsAndBytes config\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:54:58,677\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading model with pretrained weights\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:54:58,677\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading quantized model\u001b[0m\n",
            "Loading checkpoint shards: 100% 2/2 [00:07<00:00,  3.67s/it]\n",
            "[\u001b[36m2024-01-16 04:55:07,978\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Turning on model's eval mode\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:55:07,983\u001b[0m][\u001b[34mbenchmark\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:55:07,983\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Running inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:55:07,983\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Updating input shapes with model shapes\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:55:07,984\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing backend for inference\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:55:07,984\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating input generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:55:07,984\u001b[0m][\u001b[34minput-generator\u001b[0m][\u001b[32mINFO\u001b[0m] - Using text-generation task generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:55:07,985\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:55:07,985\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:55:07,985\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:55:07,985\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:55:07,985\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:55:08,969\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory used: 7079 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:55:08,969\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory reserved: 6071 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:55:08,970\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory allocated: 5719 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:55:08,970\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:55:08,970\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:55:09,077\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:55:09,077\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:55:09,077\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:56:28,992\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory used: 8121 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:56:28,992\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory reserved: 7111 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:56:28,992\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory allocated: 6759 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:56:28,992\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:56:28,993\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:56:28,993\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:56:30,954\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:56:41,271\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass latency: 2.08e-01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:56:41,271\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass throughput: 19.20 (samples/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:56:41,272\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:56:41,272\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:56:41,272\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:58:01,322\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:59:20,940\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass latency: 7.96e+01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:59:20,940\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass throughput: 50.30 (tokens/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:59:20,941\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving results\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:59:20,946\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Cleaning pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:59:20,946\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Deleting pretrained model\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:59:21,410\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Emptying CUDA cache\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:59:23,070\u001b[0m][\u001b[35mHYDRA\u001b[0m] \t#3 : benchmark.input_shapes.batch_size=8\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:59:23,237\u001b[0m][\u001b[34mlauncher\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring process launcher\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:59:23,238\u001b[0m][\u001b[34mprocess\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Launched worker process with PID 14539.\u001b[0m\n",
            "2024-01-16 04:59:26.033102: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-16 04:59:26.033148: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-16 04:59:26.034489: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-16 04:59:27.219690: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[\u001b[36m2024-01-16 04:59:30,269\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - PyTorch version 2.1.0+cu121 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:59:30,270\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - TensorFlow version 2.15.0 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:59:30,271\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - JAX version 0.4.23 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:59:30,791\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:59:30,791\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Inferred class AutoModelForCausalLM for task text-generation and model_type mistral\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:59:30,792\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Disabling gradients\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:59:30,792\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing quantization config\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:59:30,792\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing BitsAndBytes config\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:59:30,793\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading model with pretrained weights\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:59:30,793\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading quantized model\u001b[0m\n",
            "Loading checkpoint shards: 100% 2/2 [00:07<00:00,  3.65s/it]\n",
            "[\u001b[36m2024-01-16 04:59:40,196\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Turning on model's eval mode\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:59:40,201\u001b[0m][\u001b[34mbenchmark\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:59:40,201\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Running inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:59:40,201\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Updating input shapes with model shapes\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:59:40,201\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing backend for inference\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:59:40,201\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating input generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:59:40,201\u001b[0m][\u001b[34minput-generator\u001b[0m][\u001b[32mINFO\u001b[0m] - Using text-generation task generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:59:40,202\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:59:40,202\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:59:40,203\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:59:40,203\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:59:40,203\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:59:41,100\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory used: 8045 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:59:41,101\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory reserved: 7038 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:59:41,101\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory allocated: 6398 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:59:41,101\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:59:41,101\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:59:41,301\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:59:41,302\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:59:41,302\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:01:01,289\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory used: 10533 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:01:01,290\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory reserved: 9523 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:01:01,290\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory allocated: 8443 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:01:01,290\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:01:01,291\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:01:01,291\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:01:04,939\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:01:15,181\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass latency: 3.86e-01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:01:15,182\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass throughput: 20.70 (samples/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:01:15,183\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:01:15,183\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:01:15,183\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:02:35,291\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:03:55,648\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass latency: 8.04e+01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:03:55,649\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass throughput: 99.50 (tokens/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:03:55,649\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving results\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:03:55,654\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Cleaning pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:03:55,654\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Deleting pretrained model\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:03:56,123\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Emptying CUDA cache\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:03:57,917\u001b[0m][\u001b[35mHYDRA\u001b[0m] \t#4 : benchmark.input_shapes.batch_size=16\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:03:58,084\u001b[0m][\u001b[34mlauncher\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring process launcher\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:03:58,085\u001b[0m][\u001b[34mprocess\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Launched worker process with PID 15701.\u001b[0m\n",
            "2024-01-16 05:04:00.869185: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-16 05:04:00.869238: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-16 05:04:00.870624: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-16 05:04:02.054625: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[\u001b[36m2024-01-16 05:04:05,103\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - PyTorch version 2.1.0+cu121 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:04:05,104\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - TensorFlow version 2.15.0 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:04:05,105\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - JAX version 0.4.23 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:04:05,588\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:04:05,588\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Inferred class AutoModelForCausalLM for task text-generation and model_type mistral\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:04:05,588\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Disabling gradients\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:04:05,589\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing quantization config\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:04:05,589\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing BitsAndBytes config\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:04:05,590\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading model with pretrained weights\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:04:05,590\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading quantized model\u001b[0m\n",
            "Loading checkpoint shards: 100% 2/2 [00:07<00:00,  3.68s/it]\n",
            "[\u001b[36m2024-01-16 05:04:14,915\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Turning on model's eval mode\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:04:14,920\u001b[0m][\u001b[34mbenchmark\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:04:14,920\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Running inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:04:14,920\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Updating input shapes with model shapes\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:04:14,920\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing backend for inference\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:04:14,920\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating input generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:04:14,921\u001b[0m][\u001b[34minput-generator\u001b[0m][\u001b[32mINFO\u001b[0m] - Using text-generation task generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:04:14,921\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:04:14,922\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:04:14,922\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:04:14,922\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:04:14,922\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:04:16,039\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory used: 9883 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:04:16,039\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory reserved: 8875 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:04:16,039\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory allocated: 7755 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:04:16,040\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:04:16,040\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:04:16,432\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:04:16,432\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:04:16,432\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:05:37,083\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory used: 14811 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:05:37,083\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory reserved: 13801 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:05:37,083\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory allocated: 11811 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:05:37,084\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:05:37,084\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:05:37,085\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:05:44,110\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:05:54,933\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass latency: 7.44e-01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:05:54,933\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass throughput: 21.50 (samples/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:05:54,934\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:05:54,934\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:05:54,934\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:07:15,918\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:08:36,812\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass latency: 8.09e+01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:08:36,813\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass throughput: 198.00 (tokens/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:08:36,813\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving results\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:08:36,818\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Cleaning pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:08:36,818\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Deleting pretrained model\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:08:37,271\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Emptying CUDA cache\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Benchmarking BNB's NF4 with double quantization"
      ],
      "metadata": {
        "id": "Sngh9hWhOuzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "YAML_DEFAULT=\"\"\"\n",
        "defaults:\n",
        "  - backend: pytorch # default backend\n",
        "  - benchmark: inference # we will monitor the inference\n",
        "  - launcher: process\n",
        "  - experiment # inheriting from experiment config\n",
        "  - _self_ # for hydra 1.1 compatibility\n",
        "  - override hydra/job_logging: colorlog # colorful logging\n",
        "  - override hydra/hydra_logging: colorlog # colorful logging\n",
        "\n",
        "hydra:\n",
        "  run:\n",
        "    dir: experiments/${experiment_name} #The results will be reported in this directory. Note that \"experiment_name\" refers to the configuration field name \"experiment_name\" below\n",
        "  sweep:\n",
        "    dir: experiments/${experiment_name}\n",
        "  job:\n",
        "    chdir: true\n",
        "    env_set: #These are environment variable that you may want to set before running the benchmark\n",
        "      CUDA_VISIBLE_DEVICES: 0\n",
        "      CUDA_DEVICE_ORDER: PCI_BUS_ID\n",
        "  sweeper:\n",
        "    params:\n",
        "      benchmark.input_shapes.batch_size: 1,2,4,8,16 #we will try all these batch sizes\n",
        "\n",
        "experiment_name: bnb_dq-batch_size(${benchmark.input_shapes.batch_size})-sequence_length(${benchmark.input_shapes.sequence_length})-new_tokens(${benchmark.new_tokens})\n",
        "model: mistralai/Mistral-7B-v0.1 #The model that we want to evaluate. It can be from the Hugging Face Hub or local directory\n",
        "device: cuda #Which device to use for the benchmark. We will use CUDA, i.e., the GPU\n",
        "\n",
        "backend:\n",
        "  torch_dtype: float16 #The model will be loaded with fp16\n",
        "  quantization_scheme: bnb\n",
        "  quantization_config:\n",
        "    load_in_4bit: true\n",
        "    bnb_4bit_compute_dtype: float16\n",
        "    bnb_4bit_use_double_quant: true\n",
        "\n",
        "benchmark:\n",
        "  memory: true #We will monitor the memory usage\n",
        "  warmup_runs: 10 #Before the monitoring starts, the inference will be run 10 times for warming up\n",
        "\n",
        "  new_tokens: 1000 #Inference will generate 1000 tokens\n",
        "  input_shapes:\n",
        "    sequence_length: 512 #Prompt will have 512 tokens\n",
        "\"\"\"\n",
        "\n",
        "with open(\"mistral_7b_bnb_dq_ob.yaml\", 'w') as f:\n",
        "  f.write(YAML_DEFAULT)"
      ],
      "metadata": {
        "id": "PuUo-WnNOvqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!optimum-benchmark --config-dir ./ --config-name mistral_7b_bnb_dq_ob --multirun"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hg6zHtnDO9rb",
        "outputId": "cb56d928-bf15-4a75-fa7c-5a01b34aa37b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-16 05:08:43.112464: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-16 05:08:43.112511: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-16 05:08:43.113993: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-16 05:08:44.311961: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[\u001b[36m2024-01-16 05:08:47,787\u001b[0m][\u001b[35mHYDRA\u001b[0m] Launching 5 jobs locally\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:08:47,787\u001b[0m][\u001b[35mHYDRA\u001b[0m] \t#0 : benchmark.input_shapes.batch_size=1\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:08:47,955\u001b[0m][\u001b[34mlauncher\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring process launcher\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:08:47,956\u001b[0m][\u001b[34mprocess\u001b[0m][\u001b[32mINFO\u001b[0m] - Setting multiprocessing start method to spawn.\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:08:47,958\u001b[0m][\u001b[34mprocess\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Launched worker process with PID 16961.\u001b[0m\n",
            "2024-01-16 05:08:50.744322: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-16 05:08:50.744375: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-16 05:08:50.746072: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-16 05:08:51.959932: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[\u001b[36m2024-01-16 05:08:55,058\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - PyTorch version 2.1.0+cu121 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:08:55,058\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - TensorFlow version 2.15.0 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:08:55,060\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - JAX version 0.4.23 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:08:55,566\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:08:55,567\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Inferred class AutoModelForCausalLM for task text-generation and model_type mistral\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:08:55,567\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Disabling gradients\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:08:55,567\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing quantization config\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:08:55,567\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing BitsAndBytes config\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:08:55,568\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading model with pretrained weights\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:08:55,568\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading quantized model\u001b[0m\n",
            "Loading checkpoint shards: 100% 2/2 [00:07<00:00,  3.57s/it]\n",
            "[\u001b[36m2024-01-16 05:09:04,786\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Turning on model's eval mode\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:09:04,791\u001b[0m][\u001b[34mbenchmark\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:09:04,791\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Running inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:09:04,791\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Updating input shapes with model shapes\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:09:04,792\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing backend for inference\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:09:04,792\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating input generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:09:04,792\u001b[0m][\u001b[34minput-generator\u001b[0m][\u001b[32mINFO\u001b[0m] - Using text-generation task generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:09:04,792\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:09:04,793\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:09:04,793\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:09:04,793\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:09:04,793\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:09:05,618\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory used: 6091 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:09:05,618\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory reserved: 5083 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:09:05,619\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory allocated: 4904 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:09:05,619\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:09:05,619\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:09:05,619\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:09:05,619\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:09:05,619\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:10:18,293\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory used: 6261 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:10:18,294\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory reserved: 5251 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:10:18,294\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory allocated: 5096 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:10:18,294\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:10:18,294\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:10:18,295\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:10:19,239\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:10:29,288\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass latency: 9.47e-02 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:10:29,289\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass throughput: 10.60 (samples/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:10:29,290\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:10:29,290\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:10:29,290\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:11:42,133\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:12:55,457\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass latency: 7.33e+01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:12:55,458\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass throughput: 13.60 (tokens/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:12:55,458\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving results\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:12:55,464\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Cleaning pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:12:55,464\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Deleting pretrained model\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:12:55,952\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Emptying CUDA cache\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:12:57,655\u001b[0m][\u001b[35mHYDRA\u001b[0m] \t#1 : benchmark.input_shapes.batch_size=2\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:12:57,822\u001b[0m][\u001b[34mlauncher\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring process launcher\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:12:57,823\u001b[0m][\u001b[34mprocess\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Launched worker process with PID 18023.\u001b[0m\n",
            "2024-01-16 05:13:00.591864: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-16 05:13:00.591908: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-16 05:13:00.593307: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-16 05:13:01.772726: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[\u001b[36m2024-01-16 05:13:04,951\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - PyTorch version 2.1.0+cu121 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:13:04,951\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - TensorFlow version 2.15.0 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:13:04,952\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - JAX version 0.4.23 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:13:05,456\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:13:05,456\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Inferred class AutoModelForCausalLM for task text-generation and model_type mistral\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:13:05,456\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Disabling gradients\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:13:05,456\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing quantization config\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:13:05,457\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing BitsAndBytes config\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:13:05,457\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading model with pretrained weights\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:13:05,457\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading quantized model\u001b[0m\n",
            "Loading checkpoint shards: 100% 2/2 [00:07<00:00,  3.61s/it]\n",
            "[\u001b[36m2024-01-16 05:13:14,849\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Turning on model's eval mode\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:13:14,854\u001b[0m][\u001b[34mbenchmark\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:13:14,854\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Running inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:13:14,854\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Updating input shapes with model shapes\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:13:14,855\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing backend for inference\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:13:14,855\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating input generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:13:14,855\u001b[0m][\u001b[34minput-generator\u001b[0m][\u001b[32mINFO\u001b[0m] - Using text-generation task generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:13:14,855\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:13:14,856\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:13:14,856\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:13:14,856\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:13:14,856\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:13:15,682\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory used: 6257 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:13:15,683\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory reserved: 5249 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:13:15,683\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory allocated: 5017 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:13:15,683\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:13:15,683\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:13:15,730\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:13:15,730\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:13:15,730\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:14:50,621\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory used: 6846 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:14:50,621\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory reserved: 5836 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:14:50,621\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory allocated: 5586 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:14:50,621\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:14:50,622\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:14:50,622\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:14:51,767\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:15:01,884\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass latency: 1.21e-01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:15:01,885\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass throughput: 16.50 (samples/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:15:01,885\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:15:01,885\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:15:01,886\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:16:37,145\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:18:11,626\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass latency: 9.45e+01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:18:11,627\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass throughput: 21.20 (tokens/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:18:11,627\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving results\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:18:11,632\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Cleaning pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:18:11,633\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Deleting pretrained model\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:18:12,099\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Emptying CUDA cache\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:18:13,872\u001b[0m][\u001b[35mHYDRA\u001b[0m] \t#2 : benchmark.input_shapes.batch_size=4\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:18:14,038\u001b[0m][\u001b[34mlauncher\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring process launcher\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:18:14,040\u001b[0m][\u001b[34mprocess\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Launched worker process with PID 19369.\u001b[0m\n",
            "2024-01-16 05:18:16.832106: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-16 05:18:16.832147: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-16 05:18:16.833521: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-16 05:18:18.040600: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[\u001b[36m2024-01-16 05:18:21,126\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - PyTorch version 2.1.0+cu121 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:18:21,127\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - TensorFlow version 2.15.0 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:18:21,128\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - JAX version 0.4.23 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:18:21,621\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:18:21,621\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Inferred class AutoModelForCausalLM for task text-generation and model_type mistral\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:18:21,621\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Disabling gradients\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:18:21,622\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing quantization config\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:18:21,622\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing BitsAndBytes config\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:18:21,623\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading model with pretrained weights\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:18:21,623\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading quantized model\u001b[0m\n",
            "Loading checkpoint shards: 100% 2/2 [00:07<00:00,  3.72s/it]\n",
            "[\u001b[36m2024-01-16 05:18:31,263\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Turning on model's eval mode\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:18:31,268\u001b[0m][\u001b[34mbenchmark\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:18:31,268\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Running inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:18:31,268\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Updating input shapes with model shapes\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:18:31,268\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing backend for inference\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:18:31,268\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating input generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:18:31,268\u001b[0m][\u001b[34minput-generator\u001b[0m][\u001b[32mINFO\u001b[0m] - Using text-generation task generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:18:31,269\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:18:31,269\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:18:31,269\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:18:31,270\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:18:31,270\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:18:32,107\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory used: 6720 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:18:32,108\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory reserved: 5712 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:18:32,108\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory allocated: 5351 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:18:32,108\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:18:32,108\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:18:32,204\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:18:32,205\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:18:32,205\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:20:06,829\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory used: 7748 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:20:06,830\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory reserved: 6738 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:20:06,830\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory allocated: 6391 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:20:06,830\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:20:06,830\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:20:06,831\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:20:08,831\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:20:19,044\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass latency: 2.11e-01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:20:19,045\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass throughput: 19.00 (samples/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:20:19,046\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:20:19,046\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:20:19,046\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:21:53,292\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:23:27,077\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass latency: 9.38e+01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:23:27,078\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass throughput: 42.60 (tokens/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:23:27,078\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving results\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:23:27,083\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Cleaning pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:23:27,083\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Deleting pretrained model\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:23:27,539\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Emptying CUDA cache\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:23:29,242\u001b[0m][\u001b[35mHYDRA\u001b[0m] \t#3 : benchmark.input_shapes.batch_size=8\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:23:29,409\u001b[0m][\u001b[34mlauncher\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring process launcher\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:23:29,410\u001b[0m][\u001b[34mprocess\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Launched worker process with PID 20697.\u001b[0m\n",
            "2024-01-16 05:23:32.205618: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-16 05:23:32.205668: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-16 05:23:32.207377: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-16 05:23:33.405602: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[\u001b[36m2024-01-16 05:23:36,530\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - PyTorch version 2.1.0+cu121 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:23:36,530\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - TensorFlow version 2.15.0 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:23:36,532\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - JAX version 0.4.23 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:23:37,099\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:23:37,099\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Inferred class AutoModelForCausalLM for task text-generation and model_type mistral\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:23:37,099\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Disabling gradients\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:23:37,100\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing quantization config\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:23:37,100\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing BitsAndBytes config\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:23:37,100\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading model with pretrained weights\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:23:37,101\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading quantized model\u001b[0m\n",
            "Loading checkpoint shards: 100% 2/2 [00:07<00:00,  3.60s/it]\n",
            "[\u001b[36m2024-01-16 05:23:46,475\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Turning on model's eval mode\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:23:46,480\u001b[0m][\u001b[34mbenchmark\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:23:46,481\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Running inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:23:46,481\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Updating input shapes with model shapes\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:23:46,481\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing backend for inference\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:23:46,481\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating input generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:23:46,481\u001b[0m][\u001b[34minput-generator\u001b[0m][\u001b[32mINFO\u001b[0m] - Using text-generation task generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:23:46,482\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:23:46,482\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:23:46,482\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:23:46,482\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:23:46,482\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:23:47,458\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory used: 7683 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:23:47,458\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory reserved: 6675 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:23:47,458\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory allocated: 6029 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:23:47,458\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:23:47,459\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:23:47,622\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:23:47,622\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:23:47,622\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:25:22,236\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory used: 10126 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:25:22,236\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory reserved: 9116 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:25:22,236\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory allocated: 8076 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:25:22,237\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:25:22,237\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:25:22,237\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:25:25,937\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:25:36,191\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass latency: 3.88e-01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:25:36,192\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass throughput: 20.60 (samples/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:25:36,192\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:25:36,192\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:25:36,193\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:27:10,202\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:28:44,570\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass latency: 9.44e+01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:28:44,570\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass throughput: 84.70 (tokens/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:28:44,570\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving results\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:28:44,575\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Cleaning pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:28:44,575\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Deleting pretrained model\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:28:45,043\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Emptying CUDA cache\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:28:46,778\u001b[0m][\u001b[35mHYDRA\u001b[0m] \t#4 : benchmark.input_shapes.batch_size=16\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:28:46,944\u001b[0m][\u001b[34mlauncher\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring process launcher\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:28:46,945\u001b[0m][\u001b[34mprocess\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Launched worker process with PID 22037.\u001b[0m\n",
            "2024-01-16 05:28:49.702391: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-16 05:28:49.702440: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-16 05:28:49.703831: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-16 05:28:50.876207: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[\u001b[36m2024-01-16 05:28:53,960\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - PyTorch version 2.1.0+cu121 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:28:53,961\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - TensorFlow version 2.15.0 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:28:53,962\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - JAX version 0.4.23 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:28:54,454\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:28:54,455\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Inferred class AutoModelForCausalLM for task text-generation and model_type mistral\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:28:54,455\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Disabling gradients\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:28:54,455\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing quantization config\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:28:54,455\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing BitsAndBytes config\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:28:54,456\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading model with pretrained weights\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:28:54,456\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading quantized model\u001b[0m\n",
            "Loading checkpoint shards: 100% 2/2 [00:07<00:00,  3.59s/it]\n",
            "[\u001b[36m2024-01-16 05:29:03,859\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Turning on model's eval mode\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:29:03,864\u001b[0m][\u001b[34mbenchmark\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:29:03,864\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Running inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:29:03,864\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Updating input shapes with model shapes\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:29:03,864\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing backend for inference\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:29:03,865\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating input generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:29:03,865\u001b[0m][\u001b[34minput-generator\u001b[0m][\u001b[32mINFO\u001b[0m] - Using text-generation task generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:29:03,865\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:29:03,866\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:29:03,866\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:29:03,866\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:29:03,866\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:29:05,075\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory used: 9524 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:29:05,075\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory reserved: 8516 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:29:05,076\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory allocated: 7386 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:29:05,076\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:29:05,076\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:29:05,387\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:29:05,388\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:29:05,388\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:30:41,694\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory used: 14926 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:30:41,695\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory reserved: 13916 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:30:41,695\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory allocated: 11444 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:30:41,695\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:30:41,696\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:30:41,696\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:30:48,822\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:30:59,620\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass latency: 7.48e-01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:30:59,620\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass throughput: 21.40 (samples/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:30:59,621\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:30:59,621\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:30:59,621\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:32:36,000\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:34:12,309\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass latency: 9.63e+01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:34:12,309\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass throughput: 166.00 (tokens/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:34:12,309\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving results\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:34:12,315\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Cleaning pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:34:12,315\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Deleting pretrained model\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:34:12,779\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Emptying CUDA cache\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Benchmarking AWQ"
      ],
      "metadata": {
        "id": "5dyT28a5Qh3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "YAML_DEFAULT=\"\"\"\n",
        "defaults:\n",
        "  - backend: pytorch # default backend\n",
        "  - benchmark: inference # we will monitor the inference\n",
        "  - launcher: process\n",
        "  - experiment # inheriting from experiment config\n",
        "  - _self_ # for hydra 1.1 compatibility\n",
        "  - override hydra/job_logging: colorlog # colorful logging\n",
        "  - override hydra/hydra_logging: colorlog # colorful logging\n",
        "\n",
        "hydra:\n",
        "  run:\n",
        "    dir: experiments/${experiment_name} #The results will be reported in this directory. Note that \"experiment_name\" refers to the configuration field name \"experiment_name\" below\n",
        "  sweep:\n",
        "    dir: experiments/${experiment_name}\n",
        "  job:\n",
        "    chdir: true\n",
        "    env_set: #These are environment variable that you may want to set before running the benchmark\n",
        "      CUDA_VISIBLE_DEVICES: 0\n",
        "      CUDA_DEVICE_ORDER: PCI_BUS_ID\n",
        "  sweeper:\n",
        "    params:\n",
        "      benchmark.input_shapes.batch_size: 1,2,4,8,16 #we will try all these batch sizes\n",
        "\n",
        "experiment_name: awq-batch_size(${benchmark.input_shapes.batch_size})-sequence_length(${benchmark.input_shapes.sequence_length})-new_tokens(${benchmark.new_tokens})\n",
        "model: kaitchup/Mistral-7B-awq-4bit #The model that we want to evaluate. It can be from the Hugging Face Hub or local directory\n",
        "device: cuda #Which device to use for the benchmark. We will use CUDA, i.e., the GPU\n",
        "\n",
        "backend:\n",
        "  torch_dtype: float16 #The model will be loaded with fp16\n",
        "\n",
        "benchmark:\n",
        "  memory: true #We will monitor the memory usage\n",
        "  warmup_runs: 10 #Before the monitoring starts, the inference will be run 10 times for warming up\n",
        "\n",
        "  new_tokens: 1000 #Inference will generate 1000 tokens\n",
        "  input_shapes:\n",
        "    sequence_length: 512 #Prompt will have 512 tokens\n",
        "\"\"\"\n",
        "\n",
        "with open(\"mistral_7b_awq_ob.yaml\", 'w') as f:\n",
        "  f.write(YAML_DEFAULT)"
      ],
      "metadata": {
        "id": "EN2FgR8yQi6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!optimum-benchmark --config-dir ./ --config-name mistral_7b_awq_ob --multirun"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNPz5ikJQ7ME",
        "outputId": "b2b600e7-5b8f-4e79-d28e-0ef595a81be8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-16 04:17:09.135766: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-16 04:17:09.135815: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-16 04:17:09.137536: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-16 04:17:10.296696: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[\u001b[36m2024-01-16 04:17:13,735\u001b[0m][\u001b[35mHYDRA\u001b[0m] Launching 5 jobs locally\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:17:13,735\u001b[0m][\u001b[35mHYDRA\u001b[0m] \t#0 : benchmark.input_shapes.batch_size=1\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:17:13,900\u001b[0m][\u001b[34mlauncher\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring process launcher\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:17:13,900\u001b[0m][\u001b[34mprocess\u001b[0m][\u001b[32mINFO\u001b[0m] - Setting multiprocessing start method to spawn.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:17:13,903\u001b[0m][\u001b[34mprocess\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Launched worker process with PID 3440.\u001b[0m\n",
            "2024-01-16 04:17:16.677970: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-16 04:17:16.678013: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-16 04:17:16.679156: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-16 04:17:17.856686: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[\u001b[36m2024-01-16 04:17:21,506\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - PyTorch version 2.1.0+cu121 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:17:21,506\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - TensorFlow version 2.15.0 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:17:21,507\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - JAX version 0.4.23 available.\u001b[0m\n",
            "config.json: 100% 764/764 [00:00<00:00, 5.39MB/s]\n",
            "tokenizer_config.json: 100% 915/915 [00:00<00:00, 6.29MB/s]\n",
            "tokenizer.model: 100% 493k/493k [00:00<00:00, 12.2MB/s]\n",
            "tokenizer.json: 100% 1.80M/1.80M [00:00<00:00, 21.9MB/s]\n",
            "special_tokens_map.json: 100% 414/414 [00:00<00:00, 3.44MB/s]\n",
            "generation_config.json: 100% 111/111 [00:00<00:00, 836kB/s]\n",
            "[\u001b[36m2024-01-16 04:17:22,845\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:17:22,845\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Inferred class AutoModelForCausalLM for task text-generation and model_type mistral\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:17:22,845\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Disabling gradients\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:17:22,846\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing quantization config\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:17:22,846\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing AWQ config\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:17:22,870\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading model with pretrained weights\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:17:22,870\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading quantized model\u001b[0m\n",
            "model.safetensors: 100% 4.15G/4.15G [00:51<00:00, 80.7MB/s]\n",
            "[\u001b[36m2024-01-16 04:18:18,208\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Turning on model's eval mode\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:18:18,213\u001b[0m][\u001b[34mbenchmark\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:18:18,213\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Running inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:18:18,213\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Updating input shapes with model shapes\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:18:18,213\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing backend for inference\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:18:18,213\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating input generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:18:18,214\u001b[0m][\u001b[34minput-generator\u001b[0m][\u001b[32mINFO\u001b[0m] - Using text-generation task generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:18:18,214\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:18:18,214\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:18:18,215\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:18:18,215\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:18:18,215\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:18:19,073\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory used: 6108 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:18:19,073\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory reserved: 5100 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:18:19,073\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory allocated: 4927 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:18:19,073\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:18:19,073\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:18:19,175\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:18:19,175\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:18:19,175\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:19:03,666\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory used: 6278 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:19:03,666\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory reserved: 5268 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:19:03,666\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory allocated: 5118 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:19:03,667\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:19:03,667\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:19:03,667\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:19:04,982\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:19:15,176\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass latency: 1.40e-01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:19:15,177\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass throughput: 7.14 (samples/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:19:15,178\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:19:15,178\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:19:15,178\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:19:59,402\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:20:43,596\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass latency: 4.42e+01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:20:43,596\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass throughput: 22.60 (tokens/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:20:43,596\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving results\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:20:43,602\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Cleaning pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:20:43,602\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Deleting pretrained model\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:20:44,053\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Emptying CUDA cache\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:20:45,712\u001b[0m][\u001b[35mHYDRA\u001b[0m] \t#1 : benchmark.input_shapes.batch_size=2\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:20:45,881\u001b[0m][\u001b[34mlauncher\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring process launcher\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:20:45,882\u001b[0m][\u001b[34mprocess\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Launched worker process with PID 4385.\u001b[0m\n",
            "2024-01-16 04:20:48.658443: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-16 04:20:48.658487: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-16 04:20:48.659813: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-16 04:20:49.846835: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[\u001b[36m2024-01-16 04:20:52,814\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - PyTorch version 2.1.0+cu121 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:20:52,814\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - TensorFlow version 2.15.0 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:20:52,816\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - JAX version 0.4.23 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:20:53,313\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:20:53,313\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Inferred class AutoModelForCausalLM for task text-generation and model_type mistral\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:20:53,313\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Disabling gradients\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:20:53,313\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing quantization config\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:20:53,313\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing AWQ config\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:20:53,335\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading model with pretrained weights\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:20:53,335\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading quantized model\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:20:57,210\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Turning on model's eval mode\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:20:57,215\u001b[0m][\u001b[34mbenchmark\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:20:57,215\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Running inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:20:57,215\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Updating input shapes with model shapes\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:20:57,216\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing backend for inference\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:20:57,216\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating input generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:20:57,216\u001b[0m][\u001b[34minput-generator\u001b[0m][\u001b[32mINFO\u001b[0m] - Using text-generation task generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:20:57,217\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:20:57,217\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:20:57,217\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:20:57,217\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:20:57,217\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:20:58,245\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory used: 6292 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:20:58,245\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory reserved: 5284 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:20:58,245\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory allocated: 5158 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:20:58,245\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:20:58,246\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:20:58,246\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:20:58,246\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:20:58,246\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:21:43,398\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory used: 6783 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:21:43,399\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory reserved: 5773 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:21:43,399\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory allocated: 5543 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:21:43,399\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:21:43,400\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:21:43,400\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:21:45,970\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:21:56,266\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass latency: 2.74e-01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:21:56,267\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass throughput: 7.30 (samples/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:21:56,268\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:21:56,268\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:21:56,268\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:22:39,932\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:23:23,484\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass latency: 4.36e+01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:23:23,485\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass throughput: 45.90 (tokens/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:23:23,486\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving results\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:23:23,491\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Cleaning pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:23:23,491\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Deleting pretrained model\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:23:23,941\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Emptying CUDA cache\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:23:25,600\u001b[0m][\u001b[35mHYDRA\u001b[0m] \t#2 : benchmark.input_shapes.batch_size=4\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:23:25,764\u001b[0m][\u001b[34mlauncher\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring process launcher\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:23:25,766\u001b[0m][\u001b[34mprocess\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Launched worker process with PID 5111.\u001b[0m\n",
            "2024-01-16 04:23:28.572655: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-16 04:23:28.572705: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-16 04:23:28.574435: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-16 04:23:29.763429: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[\u001b[36m2024-01-16 04:23:32,735\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - PyTorch version 2.1.0+cu121 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:23:32,736\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - TensorFlow version 2.15.0 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:23:32,737\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - JAX version 0.4.23 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:23:33,256\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:23:33,256\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Inferred class AutoModelForCausalLM for task text-generation and model_type mistral\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:23:33,256\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Disabling gradients\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:23:33,256\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing quantization config\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:23:33,256\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing AWQ config\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:23:33,278\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading model with pretrained weights\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:23:33,278\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading quantized model\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:23:37,037\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Turning on model's eval mode\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:23:37,042\u001b[0m][\u001b[34mbenchmark\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:23:37,042\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Running inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:23:37,043\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Updating input shapes with model shapes\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:23:37,043\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing backend for inference\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:23:37,043\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating input generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:23:37,043\u001b[0m][\u001b[34minput-generator\u001b[0m][\u001b[32mINFO\u001b[0m] - Using text-generation task generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:23:37,044\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:23:37,044\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:23:37,044\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:23:37,044\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:23:37,044\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:23:38,043\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory used: 6722 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:23:38,043\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory reserved: 5714 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:23:38,043\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory allocated: 5621 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:23:38,044\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:23:38,044\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:23:38,355\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:23:38,356\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:23:38,356\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:24:24,230\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory used: 7804 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:24:24,230\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory reserved: 6794 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:24:24,230\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory allocated: 6385 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:24:24,231\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:24:24,231\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:24:24,231\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:24:29,331\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:24:39,961\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass latency: 5.43e-01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:24:39,961\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass throughput: 7.37 (samples/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:24:39,962\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:24:39,962\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:24:39,962\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:25:24,486\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:26:08,973\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass latency: 4.45e+01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:26:08,974\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass throughput: 89.90 (tokens/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:26:08,974\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving results\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:26:08,980\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Cleaning pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:26:08,980\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Deleting pretrained model\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:26:09,460\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Emptying CUDA cache\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:26:11,190\u001b[0m][\u001b[35mHYDRA\u001b[0m] \t#3 : benchmark.input_shapes.batch_size=8\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:26:11,359\u001b[0m][\u001b[34mlauncher\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring process launcher\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:26:11,360\u001b[0m][\u001b[34mprocess\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Launched worker process with PID 5863.\u001b[0m\n",
            "2024-01-16 04:26:14.155640: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-16 04:26:14.155696: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-16 04:26:14.157045: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-16 04:26:15.355697: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[\u001b[36m2024-01-16 04:26:18,369\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - PyTorch version 2.1.0+cu121 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:26:18,370\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - TensorFlow version 2.15.0 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:26:18,371\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - JAX version 0.4.23 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:26:18,881\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:26:18,881\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Inferred class AutoModelForCausalLM for task text-generation and model_type mistral\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:26:18,881\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Disabling gradients\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:26:18,882\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing quantization config\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:26:18,882\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing AWQ config\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:26:18,904\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading model with pretrained weights\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:26:18,904\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading quantized model\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:26:22,758\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Turning on model's eval mode\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:26:22,764\u001b[0m][\u001b[34mbenchmark\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:26:22,764\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Running inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:26:22,764\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Updating input shapes with model shapes\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:26:22,764\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing backend for inference\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:26:22,764\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating input generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:26:22,764\u001b[0m][\u001b[34minput-generator\u001b[0m][\u001b[32mINFO\u001b[0m] - Using text-generation task generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:26:22,765\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:26:22,765\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:26:22,765\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:26:22,766\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:26:22,766\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:26:24,012\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory used: 8551 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:26:24,012\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory reserved: 7543 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:26:24,012\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory allocated: 6545 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:26:24,012\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:26:24,013\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:26:24,621\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:26:24,621\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:26:24,621\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:27:10,404\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory used: 9870 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:27:10,404\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory reserved: 8860 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:27:10,404\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory allocated: 8067 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:27:10,405\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:27:10,405\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:27:10,405\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:27:20,531\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:27:31,926\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass latency: 1.08e+00 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:27:31,926\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass throughput: 7.41 (samples/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:27:31,927\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:27:31,927\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:27:31,928\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:28:17,490\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:29:03,026\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass latency: 4.55e+01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:29:03,027\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass throughput: 176.00 (tokens/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:29:03,027\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving results\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:29:03,032\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Cleaning pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:29:03,032\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Deleting pretrained model\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:29:03,503\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Emptying CUDA cache\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:29:05,182\u001b[0m][\u001b[35mHYDRA\u001b[0m] \t#4 : benchmark.input_shapes.batch_size=16\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:29:05,346\u001b[0m][\u001b[34mlauncher\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring process launcher\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:29:05,348\u001b[0m][\u001b[34mprocess\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Launched worker process with PID 6645.\u001b[0m\n",
            "2024-01-16 04:29:08.178989: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-16 04:29:08.179037: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-16 04:29:08.180366: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-16 04:29:09.362981: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[\u001b[36m2024-01-16 04:29:12,348\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - PyTorch version 2.1.0+cu121 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:29:12,349\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - TensorFlow version 2.15.0 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:29:12,350\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - JAX version 0.4.23 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:29:12,876\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:29:12,876\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Inferred class AutoModelForCausalLM for task text-generation and model_type mistral\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:29:12,876\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Disabling gradients\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:29:12,877\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing quantization config\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:29:12,877\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing AWQ config\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:29:12,899\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading model with pretrained weights\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:29:12,899\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading quantized model\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:29:16,538\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Turning on model's eval mode\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:29:16,543\u001b[0m][\u001b[34mbenchmark\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:29:16,543\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Running inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:29:16,543\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Updating input shapes with model shapes\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:29:16,544\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing backend for inference\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:29:16,544\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating input generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:29:16,544\u001b[0m][\u001b[34minput-generator\u001b[0m][\u001b[32mINFO\u001b[0m] - Using text-generation task generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:29:16,545\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:29:16,545\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:29:16,545\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:29:16,545\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:29:16,545\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:29:18,320\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory used: 11311 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:29:18,320\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory reserved: 10303 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:29:18,320\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory allocated: 8395 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:29:18,321\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:29:18,321\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:29:19,535\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:29:19,535\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:29:19,535\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:30:12,012\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory used: 13743 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:30:12,012\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory reserved: 12733 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:30:12,013\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory allocated: 11438 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:30:12,013\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:30:12,013\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:30:12,014\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:30:32,209\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:30:44,163\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass latency: 2.15e+00 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:30:44,163\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass throughput: 7.44 (samples/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:30:44,164\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:30:44,165\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:30:44,165\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:31:36,361\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:32:28,581\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass latency: 5.22e+01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:32:28,582\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass throughput: 307.00 (tokens/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:32:28,582\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving results\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:32:28,588\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Cleaning pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:32:28,588\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Deleting pretrained model\u001b[0m\n",
            "[\u001b[36m2024-01-16 04:32:29,052\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Emptying CUDA cache\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Benchmarking GPTQ"
      ],
      "metadata": {
        "id": "b7i-Pi-TvGY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "YAML_DEFAULT=\"\"\"\n",
        "defaults:\n",
        "  - backend: pytorch # default backend\n",
        "  - benchmark: inference # we will monitor the inference\n",
        "  - launcher: process\n",
        "  - experiment # inheriting from experiment config\n",
        "  - _self_ # for hydra 1.1 compatibility\n",
        "  - override hydra/job_logging: colorlog # colorful logging\n",
        "  - override hydra/hydra_logging: colorlog # colorful logging\n",
        "\n",
        "hydra:\n",
        "  run:\n",
        "    dir: experiments/${experiment_name} #The results will be reported in this directory. Note that \"experiment_name\" refers to the configuration field name \"experiment_name\" below\n",
        "  sweep:\n",
        "    dir: experiments/${experiment_name}\n",
        "  job:\n",
        "    chdir: true\n",
        "    env_set: #These are environment variable that you may want to set before running the benchmark\n",
        "      CUDA_VISIBLE_DEVICES: 0\n",
        "      CUDA_DEVICE_ORDER: PCI_BUS_ID\n",
        "  sweeper:\n",
        "    params:\n",
        "      benchmark.input_shapes.batch_size: 1,2,4,8,16 #we will try all these batch sizes\n",
        "\n",
        "experiment_name: gptq-batch_size(${benchmark.input_shapes.batch_size})-sequence_length(${benchmark.input_shapes.sequence_length})-new_tokens(${benchmark.new_tokens})\n",
        "model: flozi00/Mistral-7B-v0.1-4bit-autogptq #The model that we want to evaluate. It can be from the Hugging Face Hub or local directory\n",
        "device: cuda #Which device to use for the benchmark. We will use CUDA, i.e., the GPU\n",
        "\n",
        "backend:\n",
        "  torch_dtype: float16 #The model will be loaded with fp16\n",
        "\n",
        "benchmark:\n",
        "  memory: true #We will monitor the memory usage\n",
        "  warmup_runs: 10 #Before the monitoring starts, the inference will be run 10 times for warming up\n",
        "\n",
        "  new_tokens: 1000 #Inference will generate 1000 tokens\n",
        "  input_shapes:\n",
        "    sequence_length: 512 #Prompt will have 512 tokens\n",
        "\"\"\"\n",
        "\n",
        "with open(\"mistral_7b_gptq_ob.yaml\", 'w') as f:\n",
        "  f.write(YAML_DEFAULT)"
      ],
      "metadata": {
        "id": "Ham75TsISD8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!optimum-benchmark --config-dir ./ --config-name mistral_7b_gptq_ob --multirun"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oj2qAdaqSOjS",
        "outputId": "0248e688-1ca3-46e9-a372-5f2d45986161"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-16 05:34:18.710547: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-16 05:34:18.710597: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-16 05:34:18.712068: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-16 05:34:19.882113: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[\u001b[36m2024-01-16 05:34:23,360\u001b[0m][\u001b[35mHYDRA\u001b[0m] Launching 5 jobs locally\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:34:23,360\u001b[0m][\u001b[35mHYDRA\u001b[0m] \t#0 : benchmark.input_shapes.batch_size=1\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:34:23,525\u001b[0m][\u001b[34mlauncher\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring process launcher\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:34:23,526\u001b[0m][\u001b[34mprocess\u001b[0m][\u001b[32mINFO\u001b[0m] - Setting multiprocessing start method to spawn.\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:34:23,528\u001b[0m][\u001b[34mprocess\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Launched worker process with PID 23513.\u001b[0m\n",
            "2024-01-16 05:34:26.291593: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-16 05:34:26.291639: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-16 05:34:26.292822: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-16 05:34:27.480470: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[\u001b[36m2024-01-16 05:34:30,649\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - PyTorch version 2.1.0+cu121 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:34:30,650\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - TensorFlow version 2.15.0 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:34:30,651\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - JAX version 0.4.23 available.\u001b[0m\n",
            "config.json: 100% 799k/799k [00:00<00:00, 14.4MB/s]\n",
            "tokenizer_config.json: 100% 967/967 [00:00<00:00, 7.64MB/s]\n",
            "tokenizer.model: 100% 493k/493k [00:00<00:00, 12.3MB/s]\n",
            "tokenizer.json: 100% 1.80M/1.80M [00:00<00:00, 46.3MB/s]\n",
            "special_tokens_map.json: 100% 414/414 [00:00<00:00, 3.46MB/s]\n",
            "generation_config.json: 100% 116/116 [00:00<00:00, 737kB/s]\n",
            "[\u001b[36m2024-01-16 05:34:32,011\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:34:32,011\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Inferred class AutoModelForCausalLM for task text-generation and model_type mistral\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:34:32,011\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Disabling gradients\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:34:32,012\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing quantization config\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:34:32,012\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing GPTQ config\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:34:32,012\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading model with pretrained weights\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:34:32,012\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading quantized model\u001b[0m\n",
            "model.safetensors: 100% 4.16G/4.16G [00:23<00:00, 177MB/s]\n",
            "[\u001b[36m2024-01-16 05:34:59,892\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Turning on model's eval mode\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:34:59,897\u001b[0m][\u001b[34mbenchmark\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:34:59,897\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Running inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:34:59,897\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Updating input shapes with model shapes\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:34:59,898\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing backend for inference\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:34:59,898\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating input generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:34:59,898\u001b[0m][\u001b[34minput-generator\u001b[0m][\u001b[32mINFO\u001b[0m] - Using text-generation task generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:34:59,898\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:34:59,899\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:34:59,899\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:34:59,899\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:34:59,899\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:35:00,680\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory used: 6187 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:35:00,680\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory reserved: 5175 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:35:00,680\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory allocated: 4991 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:35:00,681\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:35:00,681\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:35:00,711\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:35:00,711\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:35:00,711\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:35:41,827\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory used: 6401 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:35:41,828\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory reserved: 5389 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:35:41,828\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory allocated: 5243 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:35:41,828\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:35:41,828\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:35:41,828\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:35:42,497\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:35:52,551\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass latency: 7.10e-02 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:35:52,552\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass throughput: 14.10 (samples/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:35:52,553\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:35:52,553\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:35:52,553\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:36:33,635\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:37:14,564\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass latency: 4.09e+01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:37:14,564\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass throughput: 24.40 (tokens/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:37:14,564\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving results\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:37:14,570\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Cleaning pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:37:14,570\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Deleting pretrained model\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:37:15,035\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Emptying CUDA cache\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:37:16,884\u001b[0m][\u001b[35mHYDRA\u001b[0m] \t#1 : benchmark.input_shapes.batch_size=2\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:37:17,053\u001b[0m][\u001b[34mlauncher\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring process launcher\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:37:17,054\u001b[0m][\u001b[34mprocess\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Launched worker process with PID 24268.\u001b[0m\n",
            "2024-01-16 05:37:19.808712: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-16 05:37:19.808769: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-16 05:37:19.810109: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-16 05:37:20.996816: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[\u001b[36m2024-01-16 05:37:24,267\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - PyTorch version 2.1.0+cu121 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:37:24,268\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - TensorFlow version 2.15.0 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:37:24,269\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - JAX version 0.4.23 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:37:24,786\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:37:24,786\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Inferred class AutoModelForCausalLM for task text-generation and model_type mistral\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:37:24,787\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Disabling gradients\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:37:24,787\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing quantization config\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:37:24,787\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing GPTQ config\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:37:24,787\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading model with pretrained weights\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:37:24,787\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading quantized model\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:37:29,250\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Turning on model's eval mode\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:37:29,255\u001b[0m][\u001b[34mbenchmark\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:37:29,255\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Running inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:37:29,255\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Updating input shapes with model shapes\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:37:29,256\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing backend for inference\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:37:29,256\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating input generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:37:29,256\u001b[0m][\u001b[34minput-generator\u001b[0m][\u001b[32mINFO\u001b[0m] - Using text-generation task generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:37:29,256\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:37:29,256\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:37:29,257\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:37:29,257\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:37:29,257\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:37:30,277\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory used: 6305 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:37:30,277\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory reserved: 5293 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:37:30,278\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory allocated: 5161 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:37:30,278\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:37:30,278\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:37:30,278\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:37:30,279\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:37:30,279\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:38:12,212\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory used: 6947 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:38:12,212\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory reserved: 5934 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:38:12,212\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory allocated: 5669 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:38:12,212\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:38:12,213\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:38:12,213\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:38:13,317\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:38:23,429\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass latency: 1.17e-01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:38:23,431\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass throughput: 17.10 (samples/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:38:23,431\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:38:23,432\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:38:23,432\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:39:05,277\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:39:46,993\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass latency: 4.17e+01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:39:46,995\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass throughput: 48.00 (tokens/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:39:46,995\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving results\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:39:47,001\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Cleaning pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:39:47,001\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Deleting pretrained model\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:39:47,469\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Emptying CUDA cache\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:39:49,326\u001b[0m][\u001b[35mHYDRA\u001b[0m] \t#2 : benchmark.input_shapes.batch_size=4\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:39:49,494\u001b[0m][\u001b[34mlauncher\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring process launcher\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:39:49,496\u001b[0m][\u001b[34mprocess\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Launched worker process with PID 24936.\u001b[0m\n",
            "2024-01-16 05:39:52.257129: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-16 05:39:52.257174: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-16 05:39:52.258504: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-16 05:39:53.468776: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[\u001b[36m2024-01-16 05:39:56,601\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - PyTorch version 2.1.0+cu121 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:39:56,602\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - TensorFlow version 2.15.0 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:39:56,603\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - JAX version 0.4.23 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:39:57,108\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:39:57,109\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Inferred class AutoModelForCausalLM for task text-generation and model_type mistral\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:39:57,109\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Disabling gradients\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:39:57,109\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing quantization config\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:39:57,109\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing GPTQ config\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:39:57,109\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading model with pretrained weights\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:39:57,109\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading quantized model\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:40:01,258\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Turning on model's eval mode\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:40:01,263\u001b[0m][\u001b[34mbenchmark\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:40:01,264\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Running inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:40:01,264\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Updating input shapes with model shapes\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:40:01,264\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing backend for inference\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:40:01,264\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating input generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:40:01,265\u001b[0m][\u001b[34minput-generator\u001b[0m][\u001b[32mINFO\u001b[0m] - Using text-generation task generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:40:01,265\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:40:01,265\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:40:01,266\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:40:01,266\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:40:01,266\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:40:02,169\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory used: 6884 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:40:02,169\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory reserved: 5872 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:40:02,169\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory allocated: 5499 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:40:02,169\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:40:02,170\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:40:02,285\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:40:02,285\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:40:02,285\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:40:43,697\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory used: 7926 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:40:43,697\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory reserved: 6914 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:40:43,697\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory allocated: 6509 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:40:43,698\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:40:43,698\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:40:43,698\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:40:45,688\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:40:55,908\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass latency: 2.11e-01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:40:55,909\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass throughput: 19.00 (samples/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:40:55,910\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:40:55,910\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:40:55,910\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:41:37,255\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:42:18,498\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass latency: 4.12e+01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:42:18,499\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass throughput: 97.10 (tokens/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:42:18,499\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving results\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:42:18,504\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Cleaning pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:42:18,504\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Deleting pretrained model\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:42:18,951\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Emptying CUDA cache\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:42:20,798\u001b[0m][\u001b[35mHYDRA\u001b[0m] \t#3 : benchmark.input_shapes.batch_size=8\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:42:20,964\u001b[0m][\u001b[34mlauncher\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring process launcher\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:42:20,966\u001b[0m][\u001b[34mprocess\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Launched worker process with PID 25600.\u001b[0m\n",
            "2024-01-16 05:42:23.771214: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-16 05:42:23.771260: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-16 05:42:23.772601: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-16 05:42:24.940214: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[\u001b[36m2024-01-16 05:42:28,069\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - PyTorch version 2.1.0+cu121 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:42:28,069\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - TensorFlow version 2.15.0 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:42:28,071\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - JAX version 0.4.23 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:42:28,583\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:42:28,583\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Inferred class AutoModelForCausalLM for task text-generation and model_type mistral\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:42:28,583\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Disabling gradients\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:42:28,583\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing quantization config\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:42:28,583\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing GPTQ config\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:42:28,584\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading model with pretrained weights\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:42:28,584\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading quantized model\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:42:32,846\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Turning on model's eval mode\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:42:32,851\u001b[0m][\u001b[34mbenchmark\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:42:32,851\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Running inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:42:32,852\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Updating input shapes with model shapes\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:42:32,852\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing backend for inference\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:42:32,852\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating input generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:42:32,852\u001b[0m][\u001b[34minput-generator\u001b[0m][\u001b[32mINFO\u001b[0m] - Using text-generation task generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:42:32,853\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:42:32,853\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:42:32,853\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:42:32,854\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:42:32,854\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:42:33,806\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory used: 7813 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:42:33,806\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory reserved: 6801 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:42:33,806\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory allocated: 6178 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:42:33,806\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:42:33,807\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:42:33,997\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:42:33,997\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:42:33,997\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:43:25,951\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory used: 10105 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:43:25,951\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory reserved: 9093 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:43:25,951\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory allocated: 8193 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:43:25,952\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:43:25,952\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:43:25,952\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:43:29,601\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:43:39,813\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass latency: 3.85e-01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:43:39,813\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass throughput: 20.80 (samples/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:43:39,814\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:43:39,814\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:43:39,815\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:44:31,517\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:45:23,200\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass latency: 5.17e+01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:45:23,201\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass throughput: 155.00 (tokens/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:45:23,201\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving results\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:45:23,207\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Cleaning pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:45:23,207\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Deleting pretrained model\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:45:23,678\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Emptying CUDA cache\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:45:25,367\u001b[0m][\u001b[35mHYDRA\u001b[0m] \t#4 : benchmark.input_shapes.batch_size=16\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:45:25,534\u001b[0m][\u001b[34mlauncher\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring process launcher\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:45:25,535\u001b[0m][\u001b[34mprocess\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Launched worker process with PID 26402.\u001b[0m\n",
            "2024-01-16 05:45:28.317193: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-16 05:45:28.317236: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-16 05:45:28.318604: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-16 05:45:29.490614: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[\u001b[36m2024-01-16 05:45:32,682\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - PyTorch version 2.1.0+cu121 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:45:32,682\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - TensorFlow version 2.15.0 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:45:32,683\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - JAX version 0.4.23 available.\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:45:33,179\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:45:33,180\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Inferred class AutoModelForCausalLM for task text-generation and model_type mistral\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:45:33,180\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Disabling gradients\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:45:33,180\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing quantization config\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:45:33,180\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing GPTQ config\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:45:33,180\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading model with pretrained weights\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:45:33,180\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading quantized model\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:45:37,298\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Turning on model's eval mode\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:45:37,303\u001b[0m][\u001b[34mbenchmark\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:45:37,303\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Running inference benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:45:37,303\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Updating input shapes with model shapes\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:45:37,304\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing backend for inference\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:45:37,304\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating input generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:45:37,304\u001b[0m][\u001b[34minput-generator\u001b[0m][\u001b[32mINFO\u001b[0m] - Using text-generation task generator\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:45:37,305\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:45:37,305\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:45:37,305\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:45:37,306\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:45:37,306\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:45:38,383\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory used: 9738 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:45:38,383\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory reserved: 8726 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:45:38,384\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass max memory allocated: 7535 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:45:38,384\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:45:38,384\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:45:38,763\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation pass peak memory\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:45:38,763\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:45:38,763\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - Tracking Pytorch CUDA devices: [0]\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:46:45,901\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory used: 14217 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:46:45,902\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory reserved: 13205 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:46:45,902\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass max memory allocated: 11563 (MB)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:46:45,902\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:46:45,903\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:46:45,903\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the forward pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:46:53,007\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking forward pass latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:47:03,890\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass latency: 7.50e-01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:47:03,890\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Forward pass throughput: 21.30 (samples/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:47:03,891\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing input for the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:47:03,891\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Moving inputs tensors to device cuda\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:47:03,892\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up the generation pass\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:48:10,779\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking generation latency and throughput\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:49:17,669\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass latency: 6.69e+01 (s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:49:17,670\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generation pass throughput: 239.00 (tokens/s)\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:49:17,670\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving results\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:49:17,675\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Cleaning pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:49:17,675\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Deleting pretrained model\u001b[0m\n",
            "[\u001b[36m2024-01-16 05:49:18,137\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Emptying CUDA cache\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Benchmarking QLoRA training"
      ],
      "metadata": {
        "id": "V0SKK4o-vAZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "YAML_DEFAULT=\"\"\"\n",
        "defaults:\n",
        "  - backend: pytorch # default backend\n",
        "  - benchmark: training # we will monitor the inference\n",
        "  - launcher: process\n",
        "  - experiment # inheriting from experiment config\n",
        "  - _self_ # for hydra 1.1 compatibility\n",
        "  - override hydra/job_logging: colorlog # colorful logging\n",
        "  - override hydra/hydra_logging: colorlog # colorful logging\n",
        "\n",
        "hydra:\n",
        "  run:\n",
        "    dir: experiments_training/${experiment_name} #The results will be reported in this directory. Note that \"experiment_name\" refers to the configuration field name \"experiment_name\" below\n",
        "  sweep:\n",
        "    dir: experiments_training/${experiment_name}\n",
        "  job:\n",
        "    chdir: true\n",
        "    env_set: #These are environment variable that you may want to set before running the benchmark\n",
        "      CUDA_VISIBLE_DEVICES: 0\n",
        "      CUDA_DEVICE_ORDER: PCI_BUS_ID\n",
        "  sweeper:\n",
        "    params:\n",
        "      benchmark.training_arguments.per_device_train_batch_size: 1,2,4,8 #we will try all these batch sizes\n",
        "\n",
        "experiment_name: qlora-batch_size(${benchmark.training_arguments.per_device_train_batch_size})\n",
        "model: mistralai/Mistral-7B-v0.1 #The model that we want to evaluate. It can be from the Hugging Face Hub or local directory\n",
        "device: cuda #Which device to use for the benchmark. We will use CUDA, i.e., the GPU\n",
        "\n",
        "backend:\n",
        "  no_weights: true\n",
        "  torch_dtype: float16 #The model will be loaded with fp16\n",
        "  #peft_model: kaitchup/Mistral-7B-v0.1-SFT-ultrachat\n",
        "  peft_strategy: lora\n",
        "  peft_config:\n",
        "    task_type: CAUSAL_LM\n",
        "  quantization_scheme: bnb\n",
        "  quantization_config:\n",
        "    load_in_4bit: true\n",
        "    bnb_4bit_compute_dtype: float16\n",
        "    bnb_4bit_use_double_quant: true\n",
        "\n",
        "\n",
        "\n",
        "benchmark:\n",
        "  memory: true\n",
        "  warmup_steps: 40\n",
        "  dataset_shapes:\n",
        "    dataset_size: 160\n",
        "    sequence_length: 256\n",
        "  training_arguments:\n",
        "    max_steps: 140\n",
        "    per_device_train_batch_size: 1\n",
        "\"\"\"\n",
        "\n",
        "with open(\"mistral_7b_qlora_ob.yaml\", 'w') as f:\n",
        "  f.write(YAML_DEFAULT)"
      ],
      "metadata": {
        "id": "-Fm4XWtCWRLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!optimum-benchmark --config-dir ./ --config-name mistral_7b_qlora_ob --multirun"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AU__0pW8XAEA",
        "outputId": "261afb43-f564-492d-8a1b-c5959a5b2eee"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-01-15 14:27:41.423277: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-15 14:27:41.423331: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-15 14:27:41.424639: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-15 14:27:42.672549: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[\u001b[36m2024-01-15 14:27:46,109\u001b[0m][\u001b[35mHYDRA\u001b[0m] Launching 4 jobs locally\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:27:46,109\u001b[0m][\u001b[35mHYDRA\u001b[0m] \t#0 : benchmark.training_arguments.per_device_train_batch_size=1\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:27:46,283\u001b[0m][\u001b[34mlauncher\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring process launcher\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:27:46,283\u001b[0m][\u001b[34mprocess\u001b[0m][\u001b[32mINFO\u001b[0m] - Setting multiprocessing start method to spawn.\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:27:46,286\u001b[0m][\u001b[34mprocess\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Launched worker process with PID 14320.\u001b[0m\n",
            "2024-01-15 14:27:49.272191: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-15 14:27:49.272246: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-15 14:27:49.273473: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-15 14:27:50.570227: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[\u001b[36m2024-01-15 14:27:53,639\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - PyTorch version 2.1.0+cu121 available.\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:27:53,639\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - TensorFlow version 2.15.0 available.\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:27:53,641\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - JAX version 0.4.23 available.\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:27:54,540\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:27:54,540\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Inferred class AutoModelForCausalLM for task text-generation and model_type mistral\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:27:54,541\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing quantization config\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:27:54,541\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing BitsAndBytes config\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:27:54,542\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading model with no weights\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:27:54,542\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating no weights model directory\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:27:54,542\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Saving pretrained config\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:27:54,543\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating no weights model\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:27:54,544\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Saving no weights model\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:27:54,545\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading no weights model\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:27:54,545\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading quantized model\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:41:29,624\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Using PEFT\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:41:29,726\u001b[0m][\u001b[34mbenchmark\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring training benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:41:29,726\u001b[0m][\u001b[34mtraining\u001b[0m][\u001b[32mINFO\u001b[0m] - Running training benchmark\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:41:29,726\u001b[0m][\u001b[34mtraining\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Updating input shapes with model shapes\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:41:29,727\u001b[0m][\u001b[34mtraining\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating dataset generator\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:41:29,727\u001b[0m][\u001b[34mdataset-generator\u001b[0m][\u001b[32mINFO\u001b[0m] - Using text-generation task generator\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:41:29,727\u001b[0m][\u001b[34mtraining\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generating training dataset\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:41:29,801\u001b[0m][\u001b[34mtraining\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating training callbacks\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:41:29,811\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Setting dataset format to `torch`\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:41:29,812\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Wrapping training arguments with transformers.TrainingArguments\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:41:29,813\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Wrapping model with transformers.Trainer\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:41:29,815\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Starting training\u001b[0m\n",
            "{'train_runtime': 33.3712, 'train_samples_per_second': 4.195, 'train_steps_per_second': 4.195, 'train_loss': 0.0, 'epoch': 0.88}\n",
            "[\u001b[36m2024-01-15 14:42:03,471\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Training finished successfully\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:42:03,472\u001b[0m][\u001b[34mtraining\u001b[0m][\u001b[32mINFO\u001b[0m] - Saving training results\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:42:03,476\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Cleaning pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:42:03,476\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Deleting pretrained model\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:42:03,990\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Cleaning temporary directory\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:42:03,991\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Emptying CUDA cache\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:42:06,036\u001b[0m][\u001b[35mHYDRA\u001b[0m] \t#1 : benchmark.training_arguments.per_device_train_batch_size=2\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:42:06,197\u001b[0m][\u001b[34mlauncher\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring process launcher\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:42:06,198\u001b[0m][\u001b[34mprocess\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Launched worker process with PID 17851.\u001b[0m\n",
            "2024-01-15 14:42:08.942876: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-15 14:42:08.942929: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-15 14:42:08.944201: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-15 14:42:10.137221: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[\u001b[36m2024-01-15 14:42:13,147\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - PyTorch version 2.1.0+cu121 available.\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:42:13,148\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - TensorFlow version 2.15.0 available.\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:42:13,149\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - JAX version 0.4.23 available.\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:42:13,983\u001b[0m][\u001b[34mbackend\u001b[0m][\u001b[32mINFO\u001b[0m] - Configuring pytorch backend\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:42:13,983\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Inferred class AutoModelForCausalLM for task text-generation and model_type mistral\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:42:13,984\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing quantization config\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:42:13,984\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Processing BitsAndBytes config\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:42:13,985\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading model with no weights\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:42:13,985\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating no weights model directory\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:42:13,985\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Saving pretrained config\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:42:13,986\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating no weights model\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:42:13,987\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Saving no weights model\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:42:13,988\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading no weights model\u001b[0m\n",
            "[\u001b[36m2024-01-15 14:42:13,988\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading quantized model\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating plots\n",
        "\n",
        "We will use the script prepared by optimum-benchmark:"
      ],
      "metadata": {
        "id": "6A3ki56xsWt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/huggingface/optimum-benchmark/ef70214a33902d33896d4edd663e08480682c05f/examples/running-mistrals/report.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mefr8ndjgBb",
        "outputId": "7d0619ea-336d-41cb-bdfc-5c8a7170f22f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-16 05:49:42--  https://raw.githubusercontent.com/huggingface/optimum-benchmark/ef70214a33902d33896d4edd663e08480682c05f/examples/running-mistrals/report.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8097 (7.9K) [text/plain]\n",
            "Saving to: ‘report.py’\n",
            "\n",
            "\rreport.py             0%[                    ]       0  --.-KB/s               \rreport.py           100%[===================>]   7.91K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-01-16 05:49:42 (68.7 MB/s) - ‘report.py’ saved [8097/8097]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flatten-dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn8YvuYg0ojy",
        "outputId": "0349259a-c8cf-43f0-dc42-359f38e2d426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flatten-dict\n",
            "  Downloading flatten_dict-0.4.2-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: six<2.0,>=1.12 in /usr/local/lib/python3.10/dist-packages (from flatten-dict) (1.16.0)\n",
            "Installing collected packages: flatten-dict\n",
            "Successfully installed flatten-dict-0.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python report.py -e experiments"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlMrDCjJ0aBF",
        "outputId": "7aad967e-255a-4e13-ea41-562d2474750e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/report.py:38: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
            "  inference_report = pd.concat(inference_reports, axis=0, ignore_index=True)\n",
            "┏━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━┓\n",
            "┃\u001b[1m           \u001b[0m┃\u001b[1m           \u001b[0m┃\u001b[1m            \u001b[0m┃\u001b[1m           \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Forward\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m           \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m  Generate\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m           \u001b[0m┃\n",
            "┃\u001b[1m           \u001b[0m┃\u001b[1m           \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Forward\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m  Forward\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Peak\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m Generate\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Peak\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m           \u001b[0m┃\n",
            "┃\u001b[1m \u001b[0m\u001b[1mExperime…\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m    Batch\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Latency\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mThroughp…\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m    Memory\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mThroughp…\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m    Memory\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mQuantiza…\u001b[0m\u001b[1m \u001b[0m┃\n",
            "┃\u001b[1m \u001b[0m\u001b[1mName     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m     Size\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       (s)\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m(samples…\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      (MB)\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m(tokens/…\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      (MB)\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Scheme\u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━┩\n",
            "│ fp16-bat… │        16 │   7.21e-01 │     22.20 │      19947 │    311.00 │      24221 │      fp16 │\n",
            "├───────────┼───────────┼────────────┼───────────┼────────────┼───────────┼────────────┼───────────┤\n",
            "│ fp16-bat… │         8 │   3.65e-01 │     21.90 │      18057 │    197.00 │      20238 │      fp16 │\n",
            "├───────────┼───────────┼────────────┼───────────┼────────────┼───────────┼────────────┼───────────┤\n",
            "│ bnb-batc… │        16 │   7.44e-01 │     21.50 │       9883 │    198.00 │      14811 │       BnB │\n",
            "├───────────┼───────────┼────────────┼───────────┼────────────┼───────────┼────────────┼───────────┤\n",
            "│ bnb_dq-b… │        16 │   7.48e-01 │     21.40 │       9524 │    166.00 │      14926 │    bnb_dq │\n",
            "├───────────┼───────────┼────────────┼───────────┼────────────┼───────────┼────────────┼───────────┤\n",
            "│ gptq-bat… │        16 │   7.50e-01 │     21.30 │       9738 │    239.00 │      14217 │      GPTQ │\n",
            "├───────────┼───────────┼────────────┼───────────┼────────────┼───────────┼────────────┼───────────┤\n",
            "│ fp16-bat… │         4 │   1.88e-01 │     21.30 │      17093 │     99.30 │      18191 │      fp16 │\n",
            "├───────────┼───────────┼────────────┼───────────┼────────────┼───────────┼────────────┼───────────┤\n",
            "│ gptq-bat… │         8 │   3.85e-01 │     20.80 │       7813 │    155.00 │      10105 │      GPTQ │\n",
            "├───────────┼───────────┼────────────┼───────────┼────────────┼───────────┼────────────┼───────────┤\n",
            "│ bnb-batc… │         8 │   3.86e-01 │     20.70 │       8045 │     99.50 │      10533 │       BnB │\n",
            "├───────────┼───────────┼────────────┼───────────┼────────────┼───────────┼────────────┼───────────┤\n",
            "│ bnb_dq-b… │         8 │   3.88e-01 │     20.60 │       7683 │     84.70 │      10126 │    bnb_dq │\n",
            "├───────────┼───────────┼────────────┼───────────┼────────────┼───────────┼────────────┼───────────┤\n",
            "│ fp16-bat… │         2 │   9.84e-02 │     20.30 │      16514 │     50.10 │      17155 │      fp16 │\n",
            "├───────────┼───────────┼────────────┼───────────┼────────────┼───────────┼────────────┼───────────┤\n",
            "│ bnb-batc… │         4 │   2.08e-01 │     19.20 │       7079 │     50.30 │       8121 │       BnB │\n",
            "├───────────┼───────────┼────────────┼───────────┼────────────┼───────────┼────────────┼───────────┤\n",
            "│ gptq-bat… │         4 │   2.11e-01 │     19.00 │       6884 │     97.10 │       7926 │      GPTQ │\n",
            "├───────────┼───────────┼────────────┼───────────┼────────────┼───────────┼────────────┼───────────┤\n",
            "│ bnb_dq-b… │         4 │   2.11e-01 │     19.00 │       6720 │     42.60 │       7748 │    bnb_dq │\n",
            "├───────────┼───────────┼────────────┼───────────┼────────────┼───────────┼────────────┼───────────┤\n",
            "│ fp16-bat… │         1 │   5.29e-02 │     18.90 │      16394 │     25.90 │      16623 │      fp16 │\n",
            "├───────────┼───────────┼────────────┼───────────┼────────────┼───────────┼────────────┼───────────┤\n",
            "│ gptq-bat… │         2 │   1.17e-01 │     17.10 │       6305 │     48.00 │       6947 │      GPTQ │\n",
            "├───────────┼───────────┼────────────┼───────────┼────────────┼───────────┼────────────┼───────────┤\n",
            "│ bnb-batc… │         2 │   1.19e-01 │     16.80 │       6649 │     25.50 │       7221 │       BnB │\n",
            "├───────────┼───────────┼────────────┼───────────┼────────────┼───────────┼────────────┼───────────┤\n",
            "│ bnb_dq-b… │         2 │   1.21e-01 │     16.50 │       6257 │     21.20 │       6846 │    bnb_dq │\n",
            "├───────────┼───────────┼────────────┼───────────┼────────────┼───────────┼────────────┼───────────┤\n",
            "│ gptq-bat… │         1 │   7.10e-02 │     14.10 │       6187 │     24.40 │       6401 │      GPTQ │\n",
            "├───────────┼───────────┼────────────┼───────────┼────────────┼───────────┼────────────┼───────────┤\n",
            "│ bnb-batc… │         1 │   7.92e-02 │     12.60 │       6470 │     17.20 │       6632 │       BnB │\n",
            "├───────────┼───────────┼────────────┼───────────┼────────────┼───────────┼────────────┼───────────┤\n",
            "│ bnb_dq-b… │         1 │   9.47e-02 │     10.60 │       6091 │     13.60 │       6261 │    bnb_dq │\n",
            "├───────────┼───────────┼────────────┼───────────┼────────────┼───────────┼────────────┼───────────┤\n",
            "│ awq-batc… │        16 │       2.15 │      7.44 │      11311 │    307.00 │      13743 │       AWQ │\n",
            "├───────────┼───────────┼────────────┼───────────┼────────────┼───────────┼────────────┼───────────┤\n",
            "│ awq-batc… │         8 │       1.08 │      7.41 │       8551 │    176.00 │       9870 │       AWQ │\n",
            "├───────────┼───────────┼────────────┼───────────┼────────────┼───────────┼────────────┼───────────┤\n",
            "│ awq-batc… │         4 │   5.43e-01 │      7.37 │       6722 │     89.90 │       7804 │       AWQ │\n",
            "├───────────┼───────────┼────────────┼───────────┼────────────┼───────────┼────────────┼───────────┤\n",
            "│ awq-batc… │         2 │   2.74e-01 │      7.30 │       6292 │     45.90 │       6783 │       AWQ │\n",
            "├───────────┼───────────┼────────────┼───────────┼────────────┼───────────┼────────────┼───────────┤\n",
            "│ awq-batc… │         1 │   1.40e-01 │      7.14 │       6108 │     22.60 │       6278 │       AWQ │\n",
            "└───────────┴───────────┴────────────┴───────────┴────────────┴───────────┴────────────┴───────────┘\n"
          ]
        }
      ]
    }
  ]
}